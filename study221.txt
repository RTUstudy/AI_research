Page 2:
2 Email: iachounta@gmail.com Address: University of Duisburg-Essen, Department of Computer Science and Applied Cognitive Science (INKO), Faculty of Engineering, Forsthausweg 2, 47057 Duisburg, Germany. ORCID ID: https://orcid.org/0000-0001-9159-0664  3 Email: mjrt@tlu.ee Address: Tallinn University, Centre of Excellence in Educational Innovation, School of Digital Technologies, Narva mnt 25, 10120 Tallinn, Estonia. ORCID ID: https://orcid.org/0000-0001-8639-1257  4 Email: eric.roldan.roa@ut.ee Address: University of Tartu, Institute of Education, Centre for Educational Technology,  ̈ Ulikooli 18, 50090 Tartu, Estonia. ORCID ID: https://orcid.org/0000-0002-7519-4933  5 Address: University of Tartu, Counselling Centre,  ̈ Ulikooli 18, 50090 Tartu, Estonia.  6 Email: aune.valk@ut.ee Address: University of Tartu,  ̈ Ulikooli 18, 50090 Tartu, Estonia.ORCID ID: https://orcid.org/0000-0001-9944-2041  1. Introduction  In this work, we aim to synthesize the perspectives of researchers and institutional stakeholders in order to understand and address dropout with the support of data. To do so, we systematically explored institutional stakeholders’ perceptions of institutional analytics (IA) as a means to reflect on and further support institutional processes, organizational structures, and facilitatory roles in the context of dropout in higher education (HE). We used a participatory approach to gather insights by conducting a series of focus groups with institutional stakeholders. We hope our work offers insights into and supports sense-making about using analytics to interpret dropout and decision-making processes on an institutional level. According to the Organisation for Economic Co-operation and Development (OECD) (Kaplan et al., 2020), graduates from higher education institutions (HEIs) enjoy tax and other benefits—such as faster economic growth and increased productivity— compared to non-holders of academic degrees (Brennan et al., 2013). Still, many students drop out of HEIs despite the benefits mentioned above and to such an extent that dropout poses a significant and costly challenge for HEIs globally (Wild & Heuling, 2020). According to Vossensteyn and colleagues (2015), every third student who enrolls in an HE program will either move to another program or leave without finishing it (Ameri et al., 2016). The completion rate of bachelor’s studies in OECD countries on average is 39% in three years and 67% in more than three years (Kaplan et al., 2020; OECD, 2019). In Estonia, the respective numbers are 34% and 59%, respectively (Kaplan et al., 2020; OECD, 2019). Thus, approximately 40% of HE students in Estonia never finish their studies. The OECD data also show that the share of students in Estonia who enrolled in bachelor’s studies and are no longer enrolled in tertiary education (and have not graduated) more than three years after the start is one of the highest, that is, 33% compared to the OECD average of 24% (Kaplan et al., 2020). This shows that high dropout rates extend beyond the first year of studies despite the labour market’s need for graduates (Brennan et al., 2013). Reducing dropout in HE was one of the key strategies in Europe’s 2020 plan (Vossensteyn et al., 2015) and a long-term goal for many HEIs (Wild & Heuling, 2020; Vossensteyn et al., 2015). To achieve this goal, it is necessary to identify the factors that may lead students to drop out (Tinto, 1975; Spady, 1970; Cabrera et al., 1992). The early identification of risk factors enables academic (or institutional) stakeholders, such as program directors and student counsellors, to take action. IA can be used to measure and analyze students’ data (e.g., grades and admission details) to gain insight into improving teaching, learning, and curriculum development. Related research has explored the impact of factors such as personal values, teaching quality, and satisfaction on student dropout in specific specializations (e.g., nursing and software engineering; Giannakos et al., 2017) or specific student groups (e.g., students from disadvantaged backgrounds; Herbaut, 2021) by analyzing quantitative data from online learning platforms (Fei & Yeung, 2015) and studying information systems (I. -   A. Chounta et al., 2020). However, to the best of our knowledge, there is no widely accepted framework or set of data-informed indicators for predicting dropout in HE. Related research focuses on dropout factors from the students’ perspective (Chen, 2012). However, it is essential to engage other institutional stakeholders (e.g., teachers and curriculum developers) in the discussion because they can influence academic policies and take proactive measures for reducing dropout by reinforcing improvements in teaching, changing course content and curriculum development, providing support services, and improving student experience (Arnold & Pistilli, 2012; Colvin et al., 2015). To implement successful strategies, a thorough understanding of the fundamental issues that affect student dropout is necessary (Behr et al., 2020). For that, we require data-driven IA to effectively identify students who may be at risk and to put in place appropriate policies for supporting these students. At the same time, we need qualitative approaches to help us interpret and contextualize data-informed insights from the perspective of institutional stakeholders (I. W. Li & Carroll, 2020). This combination can help us understand the needs of stakeholders and solidify the integration of IA solutions in practice. This paper explores the perceptions of institutional stakeholders with respect to HE dropout, factors influencing dropout, and the role of the institution in reducing dropout with the help of IA. In the following section, we provide an overview of related work, leading us to the specific research questions tackled in this paper. Next, we present the methodological approach, demonstrate the results of the analysis, and provide a contextualized discussion based on the findings. Finally, we conclude with the limitations of this work and offer directions for future research.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   180

Page 3:
2. Related Work  2.1 Dropout Predictors  We define dropping out as the ”students’ ex-matriculation from the respective study program for reasons that indicate lack of interest, motivation, capability or willingness to pursue their academic degree” (I. - A. Chounta et al., 2020; Spady, 1970). A substantial body of literature examines factors that may lead to dropout, including models introduced by Tinto (1975), Spady (1970), and Cabrera and colleagues (1992). In 1970, Spady introduced the student dropout model, which was popularized by Tinto’s student integration model. Both studies identify different characteristics, such as prior academic integration (student grades), institutional commitment, student goal commitment, and social integration. Out of the identified factors, academic integration is highlighted as the most substantial predictor (Tinto, 1975; Spady, 1970). Cabrera and colleagues (1992) offer a model that yields a different understanding, where the emphasis is on the psychological and sociological processes underlying dropout behaviour. Likewise, many studies focus on different dropout behaviour patterns. According to Crosling and colleagues (2009), dropout can arise due to the quantity and quality of the pre-information students receive regarding the admission process, the quality of teaching, the way assessments have been designed, and curriculum development. Bean (1980) argues that the quality of the institution (as measured by course dissatisfaction) and its facilities, such as classrooms/library/campus environment/food service (Patti et al., 1993; Ullah et al., 2019), and the staff-student relationship (as measured by students’ willingness to discuss learning tasks with academic staff and the level of sensitivity and availability to individual student needs) is an important factor when predicting dropout. From the social and personal perspective, Hinton (2007) argues that feelings of isolation and homesickness, accommodation and transportation issues, and especially workload-related issues might lead to dropout (Bean, 1980). Bean (1980) notes students’ background, socioeconomic factors, and residency as some of the predictors. Social engagement during university, personal characteristics (e.g., gender and family background), financial difficulties, and prior health issues are social factors that can influence dropout (Crosling et al., 2009; Willcoxson et al., 2011; I. W. Li & Carroll, 2020). Concerning students’ educational background and learning profile, several empirical studies emphasize that university entrance scores and grades (Chen, 2012), prior academic performance (Johnson, 2008; Hoffman et al., 2002), and lack of commitment to studies can impact dropout, especially in the first year of study (Willcoxson et al., 2011). According to Jaggo (2020), students who drop out during the first year of study have lower state-exam grades than students who continue their studies. Additional factors that may influence dropout are students’ problem-solving and cognitive skills (Finn et al., 2014), motivation, persistence, loss of academic self-confidence, and locus of control (Xenosa et al., 2002; Seymour & Hewitt, 1997), as well as prior academic performance (I. Li & Dockery, 2015). Some studies have focused on students’ dissatisfaction with their specialization/program, as well as dropout rates (Jung & Kim, 2018), cultural adjustments, language acquisition, and quality of the study program, as potential risk factors (Jung & Kim, 2018). Related work regarding dropout focuses on social origin (Herbaut, 2021; Georg, 2009). For example, students who come from less-advantaged backgrounds have a higher tendency to drop out (Herbaut, 2021; Georg, 2009). Further, research suggests that students may drop out due to problems related to academic activities or voluntary withdrawal (such as personal issues, health issues) (Tinto, 1975; I. W. Li & Carroll, 2020). However, if institutions fail to distinguish the core factors in their context, their strategies and policies may not have a significant impact on student success (Tinto, 1975). Our work is different from existing research since we attempt to elicit those   contextual factors that affect dropout in an HEI   to lead the design of an IA solution and   support academic stakeholders to intervene when students are at risk .  2.2 Potential Impact of Using IA to Address Dropout  Related research has focused on data-informed methods, such as learning analytics (LA), of assessing student dropout at the HEI level. Nonetheless, there is still a lack of significant evidence on the link between LA and dropout (Ifenthaler & Yau, 2020). Data-informed approaches can support institutional stakeholders in monitoring students’ progress, current status, and behaviours and potentially assessing the risks students may face. In this sense, LA can offer a potential solution for designing interventions to help students at risk. LA is the continuous and iterative process of collecting, evaluating, analyzing, and reporting institutional data to make decisions (Siemens & Long, 2011). LA can aim at different granularity levels, such as the individual, classroom, or institutional level. Here, we define IA as the application of LA methods at the institutional level (Romero & Ventura, 2020). Research on LA solutions designed to predict students at risk includes Course Signals, implemented at Purdue University (Arnold & Pistilli, 2012); prediction models, implemented at the University of Phoenix (Barber & Sharkey, 2012); Mockup dashboard for individual students, developed at the Open University UK (Wolff et al., 2014); and Student Success System, developed at the University of Wisconsin (Shehata & Arnold, 2015). Course Signals provided real-time feedback by traffic light signals (likelihood of success: green—success, red—failure, yellow—potential problems) based on student performance, such  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   181

Page 4:
as course grades, past academic history (GPA, test scores), and demographic variables (age, attempted credits, and residency). However, some researchers have been critical of Course Signals. For example, Purdue University has proved that Course Signals can increase student retention. However, according to Caulfield’s article, we can see that the retention rate has improved even without using Course Signals when looking at the data over different years. So then the question is, what has caused the retention (Caulfield, 2013)? The University of Phoenix has used data from student information systems (SISs), learning management systems (LMSs), and financial aid systems to develop a prediction model to address dropout. Follow-up cross-validation procedures verified that there is an 85%–95% accuracy of predicting whether students would pass or fail a course. These results emphasize that predictive models of identifying students at risk have higher accuracy. Sociocultural aspects may come into play when it comes to the adoption and effectiveness of the systems mentioned above (Arnold & Pistilli, 2012; Barber & Sharkey, 2012)—both systems were designed and implemented in a specific geographical and cultural context. Here, we reference these works as examples in the related literature that focus on the potential of LA to support both student success and dropout (Ifenthaler, 2015). Existing solutions typically employ descriptive and quantitative indicators, such as the following: •   students’ personal information:   gender, age, learning disability (if any), prior education history, discipline history (if any) (Daud et al., 2017; Mitra & Goldstein, 2015; Rogers et al., 2014; Gkontzis et al., 2018); •   financial and professional status:   family income, family assets, work experience, current employment status (Rogers et al., 2014; Daud et al., 2017); •   academic background:   admission scores, information regarding schools the student has attended in the past (level, type, name), enrolment options (other specializations or faculties that student has applied to), enrolment year (Daud et al., 2017; Mitra & Goldstein, 2015; Rogers et al., 2014; Gkontzis et al., 2018)); •   student engagement with LMSs and virtual learning environments:   numbers and patterns of login activity, time spent online, information regarding submission of assignments, activity on discussion forums, engagement with course materials, course and slide views, self-assessment quizzes (Conijn et al., 2017; Gkontzis et al., 2018; Nespereira et al., 2015; Okubo et al., 2017; Aguiar et al., 2014); •   course engagement and motivation:   pass/fail status, grades, assignments completed, student course history, reflections and self-assessments, number of credits enrolled in, number of lost courses, attendance statistics (Mitra & Goldstein, 2015; I.-A. Chounta et al., 2020; Carter et al., 2015; Niitsoo et al., 2014). One can argue that existing IA solutions for identifying at-risk students do not follow a systematic or standardized approach to the selection and use of data. Instead, they are developed based on data of various types and different granularity. Additionally, the significance of the indicators mentioned above depends on the context, which consequently suggests that these indicators might not apply to or be appropriate for other contexts (Ak c  ̧   apınar et al., 2019). If researchers and institutional stakeholders are interested in predicting student dropout using IA, they may need an overall view of possible dropout indicators. Specifically, institutional stakeholders’ viewpoints will be helpful since they are the ones who ultimately interpret the IA solution. This paper illustrates how to apply a systematic method to engage stakeholders and researchers in   identifying meaningful data and indicators to address dropout while still taking into account contextual factors .  2.3 Stakeholder Perspectives and Adoption of IA in HEIs  Studies in IA focus on providing solutions for issues such as providing feedback for students or predicting student dropout using student data (Nguyen et al., 2020; Herodotou et al., 2020). The adoption of these solutions depends on technological developments and the institutional communities that are typically the primary stakeholders. One of the main reasons for low adoption is the lack of involvement of stakeholders and the lack of shared understanding with the institutional community of LA (and consequently IA) services (Sun et al., 2019; Aguilar et al., 2014). Stakeholders often question the accountability and transparency aspects of the analytical services, especially when they involve advanced computational methods such as machine learning and artificial intelligence (Aguilar et al., 2014), thus discouraging the adoption of data-informed decision-making (Sun et al., 2019). To facilitate trust among the stakeholders, it is necessary to communicate what data we collect, with whom we share these data, and how they are used (Sun et al., 2019; Clow, 2012). We argue that we can effectively address dropout at the institutional level, that is, when institutional stakeholders—such as institutions’ government, curriculum developers, and administration and counselling services—have the necessary information, resources, and policies in place to support students who may be at risk. Providing stakeholders with information in terms of data is the first step. However, developing actions—and potentially cultures—in which institutional stakeholders and instructors see themselves as able to use this information to support  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   182

Page 7:
of each session, we discussed work strategies and collectively designed follow-up plans for use of an early-alert intervention dashboard. In addition, we introduced a prototype of the dashboard to the participants (I. -   A. Chounta et al., 2020; I. Chounta et al., 2019). The aim was to collect ideas on work routines with the dashboard and to design data-informed indicators that may suggest dropout risk. To analyze the participants’ input, we developed a coding scheme that reflected on the research questions of this work, and we asked two researchers to conduct the coding. Then, we analyzed the codes using latent class analysis (LCA) (Case & Light, 2011) in order to identify concepts or perceptions among academic stakeholders’ opinions regarding student dropout, factors that influence dropout, and ways to address these factors. See Figure 1 for a diagrammatic representation of the research process.  Figure 1.   A Diagrammatic Representation of the Research Process  3.1 Data Collection  The focus groups were carried out from December 2019 to February 2020 in the selected university. The focus groups took place in the stakeholders’ offices as planned after communication with them. Focus groups were face to face, and each focus group lasted around 90 minutes. Participants from the same specialization (curriculum or study program) were interviewed at the same time, resulting in five focus groups. To guide the discussions, we used an interview protocol (see Appendix B, Table 6) that was designed based on the SHEILA framework 1   and adapted to align with the research questions and sub-questions presented in Section 2.4. We divided the questionnaire into three parts. The first part (Part A) aimed to collect information about the participants, such as their work experience, rather than demographics, since we sought to understand participants’ attitudes toward dropout, which one can argue relate more to their professional expertise than their demographics (e.g., gender and age). Parts B (participants’ perception about students at risk and academic data) and C (dashboard-related questions) cover the two research questions. One researcher was responsible for conducting the semi-structured interviews and audio-recording them with the informed consent of the participants. Additionally, the researcher kept handwritten notes. We transcribed verbatim the audio files obtained and juxtaposed them with the handwritten notes.  3.2 Data Analysis  To systematically analyze the interviews, we performed content analysis on the transcriptions (Maxwell, 2012). We defined four main themes aligned to the research questions:   whether participants perceive dropout as a major concern or not ,   dropout reasons ,   strategies suggested or established to overcome dropout , and   data that can be used to predict dropout . Then, we analyzed the data to explore the relationships between themes and to identify potential sub-themes under each main theme. We carefully reviewed all transcripts before the coding and summarized focus group responses in a spreadsheet to support the formulation of the coding scheme. For each one of the themes (both the main and sub-themes), we calculated the frequencies of occurring codes. Then, two researchers (co-authors of this work) double-coded all five focus groups while systematically recording patterns and themes. We used an iterative approach where the coders performed three iterations during which they assigned codes, discussed their disagreements, and formulated arguments to support their position. After the last iteration, we calculated the Kappa coefficient for all of the categorical nodes (McHugh, 2012). We established a Kappa coefficient higher than   0 . 75   for 50 codes out of 51. For the remaining code, the Kappa coefficient was equal to   0 . 6 . Ultimately, we took into consideration all categories with Kappa higher than   0 . 6   (see Appendix A, Table 5). The reader can retrieve the final version of the coding scheme at https://tinyurl.com/f4dth23k.  1 https://sheilaproject.eu/sheila-framework/  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   185

Page 9:
4.2 Participants’ Perceptions of Student Dropout  To examine potential semantic structures and underlying themes in participant responses, we performed LCA on 51 codes generated from the content analysis (see https://tinyurl.com/f4dth23k). We started modelling with two classes and subsequently increased the number of classes by one each time to decide on a model with substantively meaningful interpretations and model fit (Foti et al., 2012). Usually, it is difficult to determine the exact model fit in LCA based on one method (Masyn, 2013), and there’s no one best method to select the latent class solution. Therefore, we calculated the Bayesian information criterion (BIC), adjusted Bayesian information criterion (aBIC), and log-likelihood (LL) and compared models to identify the best LCA model. Lower values of AIC (Akaike, 1974), BIC (Schwarz, 1978), aBIC, and LL indicate better model fit. LL compares the fit of two models by examining how much better one model predicts the data than the other. In our analysis, the BIC (621.5513) was the lowest at the one-class solution, and aBIC (534.7415) was the lowest at the five-class solution. Based on LL values (56.7), the 12-class model had a significantly better fit. Therefore, we selected model 12 as the best solution after manually analyzing and interpreting the three solutions. The 12 classes are shown in Table 3. Since we conducted our LCA analysis for all the codes without separating out RQ1 and RQ2, the classes contain results related to both RQs. However, the results of the RQs were not both available in all the classes. For example, class 1 only includes RQ1: “To what extent do the participants consider dropout as a problem for their institutes, their programs, and the university overall?” and “What do participants think are the most important dropout reasons and patterns for their institute/program?” At the same time, class 12 includes results addressing both questions: RQ1 (academic adjustment of international versus national students, family issues, political problems, qualification-oriented targets, perfectionism perceived by national students) and RQ2 (tax office data to collect personal information, qualitative perspectives of reasons for dropout). Based on participants’ statements, we manually grouped all the codes into two main categories, that is,   student aspects   and  university aspects   (Table 3). By aspects, we mean both reasons and outcomes. For instance, choosing the wrong specialty is the student’s decision and it will negatively affect the student themself. Hence, it was categorized as a student aspect. On the contrary, the low completion rate of studies will affect the HEI’s reputation, and, as such, low completion rate categories are a university aspect. Further, the strategies that the HEI can suggest or has already established (e.g., inform relevant people to take actions) are listed under the university aspects. After that, we grouped classes into three high-order levels based on class similarities (Figure 2). We interpreted the derived classes—taking into account the four subsections in the two research questions—as follows: •   Class 1—Personal dropout reasons:   Participants pointed out that students may, erroneously, believe they have learned what they need to learn and can find a good job without writing a thesis and acquiring a degree. ([ST.C2] “I think sometimes students think that they don’t need to have the thesis anymore because now they have learned what they needed to learn to have found an excellent job and they don’t want to spend the energy.”) Academic goals and motivation are the IA indicators that can be used to identify dropout reasons under this category. •   Class 2—Interest in the subjects:   Dropout may have both positive and negative impacts. Dropping out can be perceived either as correcting a wrong decision or as a problem. Wrong study choices are among the primary triggers for leaving HE. There are two main reasons behind wrong study choices: (1) Students do not understand their interest until they start the program. ([AH1.C1]: “We had somebody who had a master’s in   x . They came in to [study]   y   and then at the end of the first semester like they were doing fine, but they realized that they wanted to study for   z   and they left.) (2) Due to a low admission score, students may not be able to enroll in their preferred studies. Overall, wrong study choices are associated with negative completion intentions and dropout. •   Class 3—Curriculum alignment with satisfaction:   Misalignment of curriculum with the industry’s or the students’ expectations may lead to students’ dissatisfaction. Participants stated that students who drop out believe their education will not benefit their future life. Concerning students’ expectations, the number of dropouts for particular courses can indicate issues with the curriculum’s structure. For example, suppose students drop out of a particular course consistently. In that case, it may be necessary to re-think its position in the curriculum, that is, when and under what circumstances it is offered. At the same time, it is essential to align the purpose of the HEI and the expectations of society. One solution is to involve external stakeholders in curriculum development to understand their needs, especially for technology-related disciplines. ([ST.C2] “We probably have to include the industry people much more to understand better what they want from us.”) To understand curriculum-related issues, student feedback can be considered as an IA indicator.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   187

Page 11:
Figure 2.   Student Dropout Model reasons behind student dropout. Some students intended to work full time instead of following full-time study programs. As a result, students who spend more time working outside the HEI and have a paid, full-time career are more likely to leave the university than those who do not work or work part-time. ([AH1.C1] “If you have a job and studies, and sometimes you feel that [it is] too much.”) Therefore, employment and financial status of the students may help identify the risk of dropping out. •   Class 7—Academic integration:   This class emphasizes the importance of dropout for the institution. Even an early dropout constitutes a significant concern and can be perceived as a wasted opportunity. ([AH2.C2] “Well, it is a problem also because they took the place of somebody else.”) Students may apply for some disciplines (e.g., medicine or engineering) due to professional prestige and attractiveness. However, due to competition, ensuring a study place in these areas is difficult, and many students eventually choose another specialization where they can secure a spot. ([M.C1] “Some of them have a very clear vision that they don’t want to be profession Y, but somehow they did not get in to become profession X, and they come here. Some of them decide to graduate as profession Y. But most of them will try again and again to become a profession X.”). Students who are uncertain about their choice and their future professional image are likely to look for opportunities to change their study program. This is common in early stages of studies. There’s a clear connection between students’ decisions and credits. If the student considers transferring to another program, the time and effort they invest in their current studies may be low. Thus, the number of credits covered in the past semesters and the registered credits for the following semesters can provide valuable insights regarding dropout.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   189

Page 12:
([AH2.C2] “I looked into how many credits they have, what are the registration for next semester.”) Good admission grades may show students’ knowledge alignment with their selected specialization and can be used to identify dropouts. ([AH1.C2] “I think it would be good to see admission score.”) •   Class 8—Student well-being:   Uncertainty about the benefits of future study is a prevalent issue. If the specialization is not popular, or if students do not have a clear picture about potential career opportunities, this may lead to depression, disengagement, and eventually dropout. ([M.C1] “I think this profession is not very well known in our society. Plus their professional competency activities. What they actually can do after graduation, it is not very well known.”) Additionally, students’ workload and health issues may negatively affect their well-being while pursuing a degree. Participants requested information about students’ academic background to decide whether it contributes to student struggles. In this way, study counsellors can address the issue by providing academic support programs, especially for less-engaged students. Specifically for students who are uncertain about their future, academic support programs can help them clarify their future career paths. ([AH1.C2] “With the support programs, the student would better understand what it means to study X in this department. Because if they don’t have X at high school, they may not know anything. If they had X in high school, they might have a very different experience from what it’s here.”) •   Class 9—Faculty-student relationship:   Participants stated that students who drop out could be a wasted investment for the HEI, especially students who drop out in the last stage of their studies without completing their thesis. ([ST.C2] We invested a lot of energy by teaching them. And in principle, it was okay because the industry is benefiting from it. But, [we] don’t benefit from it.) Sometimes, students do not know how to navigate their studies, which may lead to energy loss or tiredness over time. Likewise, students get disappointed with the subject contents or teaching arrangements and conclude that university education does not correspond to their expectations. ([ST.C2] “Because they might decide, okay, let’s do something different”; “just tired from same subjects.”) In this case, HEIs can support students by arranging counselling sessions and group discussions. •   Class 10—Social support and sense of stability:   Based on the level of the curriculum, the risk of dropout may change. If the curriculum is small and the number of students is limited, dropout is a significant concern from the HEI’s perspective. At the same time, participants welcomed dropout during the first study year and due to students’ wrong choices. In this case, participants do not require any support to prevent dropout. If students fail to satisfy their social and psychological needs, it may negatively influence their academic performance and motivation. ([AH1.C1] “They may also have social problems, for example. They moved to a new place. They don’t know all the other people so long ago. These are sort of social personal skills.”) ([CS.C1] “The student has been like a little bit shy or whatever other reasons haven’t asked for proper help or guidance from their institute. And those kinds of students who were not able to ask or make it noticeable that they need help or guidance. . . .”). This situation may lead to dropout if the HEI cannot identify students’ needs. Participants stated that students’ aspirations might increase if student-supervisor mentoring relationships were encouraged. If the student is avoiding lectures or meetings with supervisors, institutional stakeholders should communicate with them and even encourage supervisors to pursue communication with them. ([AH1.C1] “It may have been even more useful than me (academic specialist) writing them, to have the supervisors write to them.”) Supervisors can effectively influence students by providing the social support, sense of stability, and belonging required in students’ academic life. Participants also stated that by keeping track of students’ yearly progress toward graduation, they can identify students in need. •   Class 11—Curriculum-related difficulties:   Students may discover that the academic program is more complicated and competitive than expected. ([CS.C3] “Sometimes there are tough subjects, and it is not good if students selected those subjects early in their studies.”) Participants reported that students might lose interest when they have to follow courses and labs regularly. Another concern is thesis-related struggling, which may contribute to students dropping out at the end of their studies. For example, students may struggle to find an exciting topic or a supervisor. Participants believed that students’ active engagement with the thesis depends on the relationship with the supervisor. On the other hand, students may give up on their thesis due to poor time management, because of disappointment due to external factors such as low grades, or because they perceive the thesis to be too demanding and challenging. Participants mentioned that to overcome study-related struggles we need to establish interventions in study skills and self-management skills. ([CS.C1] “We think about the first year; it’s really important to support them in [acquiring] study skills and self-management skills.”)  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   190

Page 13:
Due to the aforementioned difficulties, students may feel overwhelmed and may take academic leave. This does not necessarily mean that all students take academic leave due to academic life struggles. For example, there are students who use academic leave to serve in the army. Thus, before making decisions based on academic leave patterns, it is necessary to investigate the underlying reasons. ([AH1.C1] “Academic leave [is] even saying student[s] don’t have kind of strength to go on”; “For example, in the case of academic leaves, some students take leaves for serving in the army. I guess these academic leaves risk-free.”) Other than academic leaves, participants considered course registrations as a good dropout indicator. By checking the number of courses in which students registered for the next semester, the institution can be aware of whether the students have enough credits for the semester. If not, they should take action to avoid potential issues. •   Class 12—Personal and contextual aspects:   This is one of the largest latent classes, representing nearly 10 codes. The participants who reported those codes have diverse perceptions about dropout. The main perception was that dropout is a global political problem rather than a problem of HEIs. ([ST.C1] “It is not even the university’s problem. It’s a political problem everywhere in the world.”) They further elaborated that because many external factors affect dropout, it is difficult for the HEI to intervene. For example, citizenship or residency of students is usually considered a concern regarding dropout. Participants stated that international students might have a higher dropout probability than nationals because nationals are familiar and comfortable with the current lifestyle or environment. Nonetheless, there are cases where national students leave the university due to family or other commitments. ([ST.C1] “It does not make sense to mix it because the international students have indeed very different sort of problems than the Estonian students. And if we want to solve problems, then we have to know about the different kinds of issues.”) Participants believed that if the students are working or they leave the university to pursue other opportunities, there is no point in holding them back. ([AH1.C2] “The students who have chosen the wrong specialty mainly drop out during the first year. It is good for them to drop out because that would enable them to start again. There’s not a huge loss.”) However, some participants argued that it is always good to complete the degree to move forward in the job market. ([ST.C2] “It’s not a matter whether they’re getting or not a job; it’s more like being promoted or getting leadership positions.”) The participants who belong to this class did not provide positive feedback for the quantitative indicators. They said that it’s necessary to collect qualitative information to have a thorough understanding of the situation. At the same time, they pointed out that the information coming from SISs is not enough to identify students at risk. There are cases where students do not provide accurate data as input. For example, students may hide their full-time employment status since the university does not allow it. At the same time, participants agreed that it is not possible to force students to give accurate information. One possibility is to collect information from the tax office. However, at the moment, there’s no government support for the university to collect tax office data, and there are data privacy concerns as well.  5. Discussion  Our study results confirm the findings of Tinto (1975) that dropping out does not relate to only one student aspect (e.g., students’ lack of commitment or motivation). We identified 12 distinct classes based on LCA. Then, we further grouped these classes into three levels, taking into account related literature (Figure 2): institutional experience, educational goals, and personal aspects. Our findings align with and confirm existing theoretical and methodological frameworks related to reasons for student dropout (Spady, 1970; Tinto, 1975; Cabrera et al., 1992). However, our findings provide more insight into which IA indicators align with which types of dropout reasons and what kind of strategies are suitable to eliminate dropout. Barefoot (2004) emphasized the importance of focusing on institutional experience of work practices and policies and how institutional stakeholders provide advice and design course and curriculum structures. Our work supported the arguments established by Barefoot (2004), deriving four categories that prioritized students’ institutional experience of dropout. Other studies included factors such as institutional financial capability (Gansemer-Topf & Schuh, 2006)) and the size of the institution (Ryan, 2004). However, no significant relationship between the above characteristics and student dropout was established (Titus, 2006). Regarding educational goals, our findings correspond to the findings of Hovdhaugen (2009) and Pascarella and Terenzini (2005). According to Pascarella and Terenzini (2005), having clear educational goals reduces the chance of dropping out. The results of our study emphasize another aspect of this argument:   students with clear educational goals who are not satisfied by their current study program may be likely to drop out . According to Vogel and colleagues (2018), dropout may be attributed to personal aspects like demographics, family status, or health or financial concerns (Vogel et al., 2018). Our findings confirm the role of personal aspects in dropout (e.g., health  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   191

Page 14:
problems were mentioned by the participants as factors that affect students’ well-being). Besides, individual academic struggles may affect dropout, according to academic stakeholders. Having said that, the reasons for dropping out of studies cannot be viewed as a single category. Therefore, when addressing dropout, interventions cannot follow a “one size fits all” approach, and we need to establish separate plans of action to overcome institutional, educational, and personal issues. The following two sections provide an agenda for addressing student dropout by improving three main aspects identified by the study. The agenda includes what the dropout indicators are and what strategies can be implemented to overcome dropout and increase student success.  5.1 Academic Stakeholders’ Perceptions of Addressing Student Dropout  In this section, we discuss how to address institutional, educational, and personal issues in general and with the use of IA. We triangulated our findings with related literature.  5.1.1 Addressing Student Dropout by Improving Institutional Experience  Since dropout is a global issue, institutions have adopted various strategies to address it. These strategies vary based on the available resources, students’ needs, and institutional expectations. According to academic stakeholders, there are different ways that institutions can influence dropout. Previous research also emphasizes that the risk of dropping out is not an inherent quality of the student but can be a function of the interaction between student and university (West et al., 2015). Out-of-class retention programs are one way to motivate first-year students. HEIs offer various programs, such as campus orientation, community services, language programs, events to build institutional spirit, cultural events, and programs for finding lodging (Barefoot, 2004). The main goal of such programs is to increase social and institutional integration. Ongoing orientation programs and seminar courses can enhance the likelihood of retention, especially for first-year students, who get the opportunity to adjust to and integrate into the new academic environment. A “sense of belonging” is another dimension where HEIs can support students by providing academic and social support under challenging situations. Learning communities, that is, small communities where students can register for the same courses, are another type of orientation program, suggested by Tinto (1975), that supports interaction among students and provides several other benefits, such as students’ being able to track deadlines and receive personal support. Study and self-management programs can improve students’ study habits and time management skills and introduce them to campus resources (library, help centres). If the university encourages students to achieve, they will be motivated to complete their studies. Many HEIs are establishing retention initiatives for at-risk students by offering “early-alert” interventions. For example, at the beginning of the semester, poorly performing students are contacted and referred to tutoring or counselling services (Barefoot, 2004). HEIs can potentially focus on retention by appointing specialized staff whose primary responsibility is student dropout. Timely support and feedback are necessary to motivate these students. Related studies emphasize the importance of feedback, but at the same time, these studies stated that 50% of teachers do not provide students with feedback (Barefoot, 2004). Based on the participants’ statements, communication between students and supervisors can positively impact students’ decisions. If teachers and supervisors can support interactions with students, students’ institutional experience and persistence may improve.  5.1.2 Addressing Student Dropout by Supporting Educational Goals  If students are dropping out due to particular study subjects or wrong study choices, identifying those students at the beginning of their studies can help to guide them. Motivation and student choices are essential to understanding dropout and transfer. Tinto (1975) states that students’ commitment to a particular institution or personal educational goals has a strong predictive power. High-performing students may leave the university due to uninteresting (boring or not satisfying) subjects, lack of academic challenges, and the desire to transfer to another program. To overcome such issues, HEIs need to find strategic approaches to influence student transfer or dropout, such as restructuring study programs. According to Tinto (1975) and Pascarella and Terenzini (2005), students’ study behaviour, that is, students’ activity inside and outside the classroom, has a significant impact on transfers. However, student engagement does not depend only on the students. HEIs should also strive to influence students’ attitude toward learning by facilitating a learning environment that satisfies students (Hovdhaugen, 2009). HEIs should investigate methods that encourage student activity and engagement, especially by promoting communication and addressing misconceptions.  5.1.3 Addressing Student Dropout by Offering Support to Overcome Personal Issues  Other personal issues that may contribute to student dropout are the pursuit of better opportunities and students’ inability to cope with courses due to lack of background knowledge or motivation. For students who struggle with learning difficulties and extreme workload, universities can intervene to help. In some cases, HEIs can support students with personal decisions. For example, if the university can understand why students are demotivated and transfer to other disciplines, counselling sessions  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   192

Page 15:
might be helpful. If students are suffering from mentally disturbing issues, individual study plans and academic leaves would be beneficial (Worsley et al., 2020). Finally, financial aid, such as subsidized loans and scholarships, may help students focus on studying (Chen, 2012).  5.2 Academic Stakeholders’ Perceptions of the Use of IA for Addressing Student Dropout  Early identification and deep understanding of dropout are essential for catering to students with the right solutions at the right time. IA can provide reliable predictions to address at-risk students. Further, IA can make adaptive and personal planning recommendations by monitoring student behaviours. To promote effectiveness, IA should target institutional facilities as well where student behaviour occurs (Sønderlund et al., 2019). At the moment, universities cannot address the issue due to the lack of information on how to address dropouts’ problems. In other words, there is no way to retrieve the information in a meaningful way, such as which students intend to drop out, when, and why. Therefore, IA is a solution to overcome this issue.  5.2.1 What IA Can Offer to Overcome Issues Related to Institutional Experience  The main aim of using IA to improve the institutional experience is to identify students’ issues and provide appropriate guidance. In brief, there are four main strategies to support retention of different student populations (West et al., 2015): (1) at the end of the semester, to inform or reach out to students who failed courses; (2) during the semester, to provide focused outreach to identify under-engaged, under-performing, and over-challenged students; (3) at the beginning of studies, to identify students potentially at risk and provide targeted support and development opportunities; and (4) in general, to inform course design and delivery. Nevertheless, to provide useful advice, IA should learn under which circumstances students need support. Some students may perform well overall, but they decide to drop out due to a specific subject. In such situations, it is necessary to have a comprehensive look at   curriculum   to understand specific subjects that may pose difficulties for the majority of students (De Silva et al., 2020). If IA can provide information about the curriculum—including risk indicators, such as completion time and retention rates—program directors can take further action. Other than curriculum-level improvements, instructors can provide personalized comments and feedback to motivate students and use early-alert tools to improve   course  design and delivery (Star & Collette, 2010). The instructor-student relationship can further improve if IA can inform the instructors when students behave differently from peers (Tarmazdi et al., 2015). To improve the   overall institutional experience , HEIs should consider offering dual support to teachers and students alike. Program directors may take action to oversee student issues without limiting them to one aspect, such as learning issues. This can be achieved by using machine learning and predictive modelling to identify various factors or aspects that affect dropout (Arnold & Pistilli, 2012; I. -   A. Chounta et al., 2020) and then presenting this information to institutional stakeholders in the form of feedback on their curriculums. This feedback can be used to guide students to seek help or suggest changes the students have to make to achieve their goals. In many situations, students are not fully aware of unproductive behaviour patterns, and, even if they are aware of them, they do not know how to react or change them. Thus, with IA, the HEI can provide an action-oriented approach, personalized for each student’s needs early in their studies. When implementing IA solutions, it is necessary to consider the data collection processes we employ. In terms of data requirements, we need to consider two aspects. The first one is to explore existing data sets, such as data available in SISs or LMSs. The second is to combine quantitative with qualitative data, such as student feedback, to synthesize and triangulate information from various sources. Based on the participant statements, student data, such as course details, grades, and registrations (presented in Table 4), can be used to assess the dropout risk.  Table 4.   Dropout Predictors Related to Institutional Issues Class name   Dropout predictors mentioned by participants Social support and sense of stability   Study results Curriculum-related difficulties   Number   of   times   students   get   registered   for courses, academic leaves Faculty-student relationship   Feedback Institutional support of students’ goal commitment   Study priorities/choices made in the admission  5.2.2 What IA Can Offer to Overcome Issues Related to Educational Goals  Curriculum improvement is one of the main suggestions for avoiding dropout related to educational goals. However, curriculum improvement is a long-term process. A short-term solution is to identify students who are likely to drop out. Previous literature does not explicitly suggest how to do this. We argue that we can use IA by designing computational predictive models that take into account factors representing educational goals such as admission score and low completion rates, credits for next and previous semester, subsequent semester registrations, and so on.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   193

Page 16:
If most students are dissatisfied with the curriculum, it is necessary to discover why and then to improve the curriculum in the long term. When implementing an IA solution to overcome problems at the course level, we need to consider data related to course enrolment, student grading, and student progress. Program-level data, such as course selection, curriculum maps, and student outcome matrix regarding students’ development of core competencies, could also be used. For example, the risk management model developed by Wong and colleagues (2016) shows interactive visualizations of student flows through the academic programs, and the visualized analytics of core competencies helps students to learn their competency attainments (Chou et al., 2015), while Hilliger and colleagues (2020) reflect on students’ core competencies and proficiency levels. With the support of such tools, students may understand and reflect on how their study program and their own competencies match their work-life goals. Further, program directors can receive insight into the areas that need improvement, such as identifying the curriculum’s impact on job opportunities and aligning with industry, competitor, and social requirements.  5.2.3 What IA Can Offer to Overcome Personal Issues  We need to identify the student population correctly to support students with everyday personal issues. However, due to privacy issues, providing data-driven solutions to address students’ problems is unattainable. To that end, we can conduct surveys to collect data on students’ opinions of personal problems. For example, HEIs can model real-time teaching and learning activities by collecting static and dynamic data of learning behaviour and content for students who are struggling with learning. These feedback loops can enable student monitoring and positively influence students’ relationship with the institution. Since students’ generic skills change over time, it is important to monitor learners’ progress and level of competencies continuously. Then, intelligent interventions based on the real-time modelling procedures can be considered as a solution for struggling students (bin Mat et al., 2013; Papamitsiou & Economides, 2015; Ak   c  ̧   apınar et al., 2019). Eventually, HEIs can potentially provide personalized feedback about students’ strengths and weaknesses as well as guidance for support services, such as tutoring, mentoring, and learning communities. When implementing IA to support students’ personal aspects, one of the critical considerations is interpreting statistical measures depending on the student-related context value and culture. Therefore, it is necessary to focus on constructive approaches, such as benchmarking, when making decisions. For example, the healthcare industry uses analytics to evaluate service effectiveness by considering the average time patients stay in the hospital. But the extended stays can be interpreted as either “ineffective” or “high quality” (patients not discharged until fully recovered). This simple indicator can convey different meanings in different contexts. Likewise, those arguments apply to social dimensions in education. For example, if we select the international-national cluster as a dropout predictor, one may argue that international students tend to drop out due to the problems they face because they have moved to another country (such as difficulties in finding a place to stay or financial issues). At the same time, the argument could be reversed, because international students are much more focused on their studies than students in the host country since they are motivated to return to their home country having succeeded.  5.3 Theoretical and Practical Implications  Based on the agenda we proposed, HEIs can gain understanding of the IA solutions that they should develop and the actions to take in different circumstances, such as dropout related to personal, educational, or institutional aspects. Even though there are previous studies related to identifying dropout factors, this study is significant because it provides insights into institutional stakeholders’ perceptions of IA solutions, data, and the steps to address dropout. These insights further confirm the added value of IA to different stakeholders and the ample possibilities for action that IA can provide to institutional stakeholders. Based on the results, we can clearly understand that institutional stakeholders are willing to accept guidance on the decisions they must make when students are at risk of dropping out. Therefore, the suggestions and recommendations provided by this study and the rationale behind them can improve IA’s acceptability within HEIs and among institutional stakeholders. While earlier research has reported the importance of early-alert interventions, the lack of research on institutional stakeholders’ viewpoints in IA applications is a substantial concern (West et al., 2020). Previous literature highlights the importance of stakeholder perspectives on the use of IA. Although the IA developments are promising (Luzeckyj et al., 2020), those initiatives have not been widely adopted due to the lack of understanding among the institutional stakeholders who are going to use them. Even current studies related to stakeholder perceptions mostly consider the student’s perspective (West et al., 2020). However, students are not necessarily capable of solving their issues by themselves, so the institution should be involved in helping them. Thus, this study shows that early-alert interventions serve the needs of institutional stakeholders and that correct interventions can positively impact student success and reduce dropout. Another challenge in developing IA solutions is to identify the indicators to be considered (Vossensteyn et al., 2015). As a solution, this work promotes a shared understanding and agreement between researchers and institutional stakeholders when developing early-alert interventions to identify students at risk. This shared understanding can be reached by having institutional stakeholders share their needs and perceptions related to dropout indicators, inform designers to build IA solutions, and further evaluate researchers’ solutions.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   194

Page 17:
Related studies conducted in Estonia (I. -   A. Chounta et al., 2020; Niitsoo et al., 2014; Kori et al., 2016) mainly focus on student-facing dropout factors, that is, on factors that relate to students’ practice, background, or performance and do not take into account institutional aspects. Our findings confirm that student dropout is often related to a combination of reasons, including individual and curriculum-level factors, as suggested by Kori and colleagues (2016) and I.-A. Chounta and colleagues (2020). At the same time, institutional stakeholders point out that certain indicators, such as time spent on studying or prior performance (Niitsoo et al., 2014), may provide insight into the risk of dropping out but should be interpreted with caution and in context. Finally, it is essential to point out that human intervention is always necessary when making decisions and interpreting data to address student dropout based on the IA solutions. Not all dropout can be interpreted as an adverse decision. For example, voluntary dropout can allow the student to follow new possibilities, such as finding a more appropriate path. A practical implication is that universities should be careful in making decisions based on the IA outcomes that benefit students in achieving or acquiring a degree. With early-alert interventions, institutional stakeholders can look at student status and send reminders for student tasks (e.g., course registrations, payments). If the student cannot do the necessary activities even after being informed, institutional stakeholders can talk with them and help them. Our findings support, validate, and strengthen related research. In addition, we envision our study contributing to the scaling of IA across HEIs by addressing issues related to collecting, analyzing, and reporting data and to institutional processes, organizational structures, and facilitatory roles that should be further developed at the institutional level to address dropout in HE. In particular, our study highlights the following two points: (a) that institutional stakeholders perceive as crucial the use of IA solutions to support their decision-making, and (b) that institutional stakeholders are aware of the potential of IA and perceive IA as a valuable tool for addressing dropout.  6. Conclusion  Identifying students who may drop out from HEIs and reducing dropout are important and challenging tasks for HEIs. Our study explores HEI stakeholders’ perceptions of dropout and the established and suggested strategies for addressing dropout in HE. HEI dropout cannot be interpreted in isolation from contextual factors. According to our findings, three main factors contribute to dropout: institutional experience, educational goals, and personal aspects. Some of them can be influenced by HEIs, but others are beyond their scope (e.g., personal and social factors). Then we mapped the problems to be addressed (according to participants) about existing theoretical and data-driven solutions to support evidence-based decision-making in relation to dropout management. Finally, building on the stakeholders’ perceptions of dropout, we proposed a participatory agenda for IA decision-making considering various stakeholder groups (program directors and academic specialists). Due to the participatory and focus group approach, participants discussed different strategies coming from different perspectives. In addition, we saw that program directors and academic specialists had different understandings of and perspectives on who could be at risk of dropout, why it is crucial, and how to address it. We envision the proposed agenda enabling HEIs to effectively address the policy issue of student dropout to help them implement institutional processes and develop facilitatory roles among the stakeholders dealing with personal, educational, and institutional issues affecting dropout. We argue that this work can provide insights into the use of IA to design data-enhanced solutions, such as institutional dashboards; to address student dropouts; and to provide the means to institutional stakeholders for timely, evidence-based decision-making. This study is a part of a large-scale study that plans to develop an early-warning system. Therefore, we aim to discuss the implications of our results with the developers to decide how to incorporate identified indicators into the IA solution and include information in the intended dashboard. While previous research often focuses on single and specific dropout problems (Nguyen et al., 2020), we provide a broader view, identifying and addressing dropout factors at the institutional level that may be transversal to different institutions. Following the recommendation by Ifenthaler (2020), this paper further explores why and how students drop out and how IA can help to identify those cases at an early stage. In this study, we focused on institutional stakeholders, since they are the ones who provide support strategies for students to remain and succeed in their studies. Moreover, by working with different types of students for long periods, they have a good understanding of the factors that may influence student dropout. Therefore, we perceive institutional stakeholders as an important source of information for identifying students at risk and designing appropriate strategies for addressing dropout. Furthermore, the new technological innovations can be unclear and can disrupt existing practices that institutional stakeholders follow. However, to obtain a thorough understanding and develop a robust and holistic strategic approach for addressing dropout in HE, future research should also investigate the perspectives of students, instructors, and developers. In future work, we also plan to address the limitations of this study. Due to the relatively low sample size, the findings cannot be generalized across the HE sector. Therefore, we aim to validate and generalize our findings by expanding this work to include other stakeholders and institutions as well as collecting additional data. Further, cross-validating  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   195

Page 25:
Code   R1   R2   Kappa  Tax Office Data to Collect Personal Information   1   1   1 Qualitative Perspectives of Reasons for Dropouts   2   2   1 Adapt /Raise awareness to the student background   1   1   1 Identify Study Priorities/Choices Made in the Admission   4   4   1 Academic Leaves   7   6   0.72 Admission Score   1   1   1 Extracurricular Over-Involvement   2   2   1 Credits for Next and Previous Semester   2   2   1 Low Grades and Failed courses   3   3   1 Next Semester Payments   1   1   1 Academic Adjustment of International-National Students   2   2   1  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)

Page 26:
Appendix B: Interview Protocol  The time for the activity is about 90 minutes. • Prior to the focus group session  –   Thank participants for their participation, remind them of the purpose and context of the activity  –   Introduce them to the participatory approach challenge of the workshop  –   Give details about duration, and data collection, introduce and walk through the consent form, ask for permission to record  –   Ask whether there are any questions • During the focus group session  Table 6.   Interview Protocol SHEILA Aspects   Questions PART A: Information about participants – Please introduce yourself Duration: 10 minutes Identify key stakeholders   • Information about their work (experience, requirements) • What is your background • How exactly does your position relate to the *name* study curriculum • How long are you in this position Information about their work (experience, requirements) PART B: Participants’ perception about student’s at risk and academic data Duration: 45 minutes Map political context and identify desired behaviour changes General stance regarding students at risk (is it a problem? To what extent should we bother, how could we help) •   Do you worry about students’ dropping out of their studies? Would you still be willing to help in their studies? Why?  –   If yes: Do you usually try to find out the dropout rates for your curriculum? Is there any tool support? Does it worry you? Do you try to find out why? What do you do about it? Do you intervene? What kind of information would you like to have about the students who might be at risk?  –   If no: why not? What should we do -or not do -instead? Would you still be willing to receive information about students’ dropout rates or not?  –   If yes: proceed  –   If not: what was your reason for participating in this workshop?  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)

Page 27:
Develop engagement strategy Activity: if I gave you raw data, which are the students you think could be in trouble? How do they envision that data from SIS could help them do their work? • Do you have access to students’ data at SIS (what kind of data)? • Do you inspect students’ data?  –   If yes: how often? What do you usually look for  –   If not: why not •   What additional information could be meaningful? (stats about population per year, overall, historical data) [brainstorming] PART C: The Dashboard Duration: 30 minutes Analyze internal capacity to change Introduce the dashboard, show the assessments over the years and the visualizations • Can you find the students’ who are at risk and provide potential interventions? •   How easy is it to review the information presented by the dashboard? (show only students’ at risk and skip the “safe”?) Does it provide you with the information you want? •   The effectiveness of reasoning (wording, potential interpretations)? Is it helpful for you? Should this information focus on the actual metrics or should it provide pedagogical reasoning - for example, a student who might be registered in many courses in comparison to their classmates, might be overloaded and therefore at risk? •   What about the model’s confidence for the assessments it provides? Would you like to have information about the model’s accuracy? How would it affect your practice? Establish monitoring and learning frameworks Ethical considerations •   What are potential interventions? Let’s assume that these assessments are available to you and the model’s predictive accuracy is acceptable.  –   What would you do for the students who have a high risk of dropping out?  –   What would you do for the students who have a medium risk of dropping out? •   Cost and efficiency: what would the additional cost of such interventions be for you [ resources]?  –   What would be the potential dangers in case of an error? For example, a low-risk student who is predicted as high? • After the focus group  –   Summary: summarize notes and ask for additional comments  –   Thank again, ask any questions, give contact details.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)

Page 1:
Volume 9(2), 179–201. https://doi.org/10.18608/jla.2022.7507  Toward an Institutional Analytics Agenda for Addressing Student Dropout in Higher Education: An Academic Stakeholders’ Perspective  Liyanachchi Mahesha Harshani De Silva 1 , Irene-Angelica Chounta 2 , Mar ́ ıa Jes  ́ us Rodr ́ ıguez-Triana 3 , Eric Roldan Roa 4 , Anna Gramberg 5 , Aune Valk 6  Abstract  Although the number of students in higher education institutions (HEIs) has increased over the past two decades, it is far from assured that all students will gain an academic degree. To that end, institutional analytics (IA) can offer insights to support strategic planning with the aim of reducing dropout and therefore of minimizing its negative impact (e.g., on students, academic stakeholders, and institutions). However, it is not clear how institutional stakeholders can integrate IA in their practice to overcome academic-related issues and to offer support to students who struggle to achieve their academic goals. To address this gap, we conducted focus groups with 13 institutional stakeholders of an Estonian university. By analyzing the focus group data, we identified three main categories of factors influencing dropout from the perspective of institutional stakeholders: (1) institutional experience, (2) educational goals, and (3) personal aspects. We discuss our findings from an institutional perspective with the aim of reflecting on institutional processes, organizational structures, and facilitatory roles in the context of dropout in higher education (HE). We argue that IA can provide insights into students’ institutional experience, educational goals, and personal aspects to further support decision-making on the institutional level. We envision our findings contributing to a participatory agenda for the design, implementation, and integration of IA solutions focusing on addressing dropout in HE.  Notes for Practice  •   Dropout in higher education (HE) is a worldwide societal problem, with a negative impact on the reputation and function of higher education institutions (HEIs).  •   Institutional analytics (IA) is a promising approach for addressing dropout in HE.  •   In this article, we identify reasons students drop out and map IA solutions that can inform HEIs’ strategic planning.  •   Results suggest that focusing only on maximizing student performance does not help reduce dropout. Beyond classic indicators based on student academic history or engagement, other factors such as curriculum and institutional and social support should be considered to predict student retention.  •   To reduce dropout, HEIs can implement IA solutions such as student dropout prediction models, competence-based models, or intelligent recommender systems that propose interventions to support students’ academic performance or overcome their academic struggles.  •   Institution-wide mentoring programs, introductory courses, counselling, and curriculum improvements have great potential for overcoming dropout rates that have been identified through IA.  •   While IA solutions are evidence based, there is a need for contextualization. As such, we envision that a human should mediate the interpretation of the analysis before triggering any intervention.  Keywords  Institutional analytics, student dropout, student success, higher education institutions  Submitted:   01/06/2021 —   Accepted:   11/04/2022 —   Published:   31/08/2022  Corresponding author   1 Email: mahesha@tlu.ee Address: Tallinn University, School of Digital Technologies, Narva mnt 25, 10120 Tallinn, Estonia. ORCID ID: https://orcid.org/0000-0003-0819-9846  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   179

Page 5:
at-risk students and actively work toward this goal is necessary (Yeager et al., 2019; Howell et al., 2018). Thus, exploring and documenting stakeholder perceptions is essential in order to increase the efficacy of IA solutions (Falc    ̃ ao et al., 2020; Dollinger et al., 2019; Tsai & Gasevic, 2017). Existing research focuses on stakeholders’ perspectives on challenges and needs for LA adoption (Hilliger, Ortiz-Rojas, Pes    ́ antez-Cabrera, Scheihing, Tsai, Mu    ̃ noz-Merino, Broos, Whitelock-Wainwright, & P    ́ erez-Sanagust  ́ ın, 2020; Beer & Lawson, 2017); policies for collecting, analyzing, and protecting data (Hilliger, Ortiz-Rojas, Pes    ́ antez-Cabrera, Scheihing, Tsai, Mu    ̃ noz- Merino, Broos, Whitelock-Wainwright, Ga   ˇ sevi    ́ c, et al., 2020; Colvin et al., 2015); teaching staff expectations of LA services (Kollom et al., 2021); and the use of early-warning systems by academic advisors (Aguilar et al., 2014). Herodotou and colleagues (2020) investigate students’ perspectives on academic failure in relation to distance learning. Therefore, related to face-to-face/university-based learning, most of the previous studies focus on students’ and teachers’ perceptions of LA implementation in general (Hilliger, Ortiz-Rojas, Pes    ́ antez-Cabrera, Scheihing, Tsai, Mu    ̃ noz-Merino, Broos, Whitelock- Wainwright, & P    ́ erez-Sanagust  ́ ın, 2020; Kollom et al., 2021) but not on a specific context such as LA adoption to reduce student dropout (Falc ̃ ao et al., 2020). Several frameworks and instruments prioritize and focus on ways to support LA and IA adoption. In this line of research, the framework ”supporting higher education to integrate learning analytics” (SHEILA) proposes materials, such as protocols for conducting surveys, interviews, and focus groups, for exploring stakeholder needs for LA services (Tsai et al., 2017) and for supporting stakeholders’ engagement. Related works explore applying the SHEILA framework to identify stakeholders’ needs related to LA adoption (Hilliger, Ortiz-Rojas, Pes    ́ antez-Cabrera, Scheihing, Tsai, Mu    ̃ noz-Merino, Broos, Whitelock-Wainwright, & P    ́ erez-Sanagust  ́ ın, 2020). Similarly, LA-DECK—a card-based co-design tool—was specifically developed to support the inter-stakeholder design of LA innovations (Alvarez et al., 2020). The approach proposed by LA-DECK supports different stakeholders, even non-technical stakeholders or stakeholders without data-related knowledge, to shape LA developments (Vezzoli et al., 2020). For example, it contains cards related to learning objectives, analysis methods, user interfaces, developer tools, data sources, privacy, and so on, which users can select. Other participatory and socio-technical approaches, such as co-design workshops accompanied by interviews, observations, or focus groups, have been explored as a means to integrate end-users’ input into the technology creation process (Liaqat et al., 2018; Gilliot et al., 2018). Participatory and socio-technical approaches allow practitioners to reflect on what they are doing, why they are doing it, and how things could be done differently, and help researchers and developers understand the implications of their work (Liaqat et al., 2018; Gilliot et al., 2018). Few large-scale studies provide an agenda for LA adoption based on stakeholders’ perspectives. These studies focus on how to use LA solutions to improve teaching quality, student experience, and student learning outcomes (Colvin et al., 2015), as well as what kinds of leadership approaches are suitable in the process of LA implementation (Dawson et al., 2018). The other agendas suggest data-informed solutions for addressing dropout based mainly on existing literature (Ifenthaler & Yau, 2020; West et al., 2015). However, to the best of our knowledge, there is no agenda that identifies, documents, and evaluates strategies that institutional stakeholders can design and that can be enhanced by IA solutions suitable for different types of dropout-related matters in HEIs. Our contribution aims to address this gap by actively   involving institutional stakeholders in the design of an agenda   that brings together the reasons behind dropout, data-informed solutions, and the design of institutional strategies.  2.4 Research Questions  The main research question we aim to address is as follows:   What are the perceptions of institutional stakeholders with respect to HE dropout? That is, what factors influence dropout, and what is the role of the institution in reducing it with the help of IA?  Under the main research question, based on related work, we formulated the following sub-questions: •   RQ1:   What are academic stakeholders’ perceptions of student dropout? To answer this question, we gathered information on the following:  –   To what extent do academic stakeholders consider dropout to be a problem for study programs and the HEI overall?  –   What are the most important reasons for dropout and patterns for study programs according to academic stakehold- ers? •   RQ2:   What are academic stakeholders’ perceptions of the use of IA to address student dropout? To answer this question, we gathered information on the following:  –   What data do academic stakeholders use (if any) to address dropout, or what data do they perceive as potentially informative for identifying future dropouts?  –   What strategies, policies, or individual practices have been established to address dropout in study programs?  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   183

Page 6:
3. Methodology  We conducted this research as part of a large-scale project that aimed to develop an institutional dashboard to support academic stakeholders (students, teachers, counselling services, administration, and HEI government). The project was carried out in a public Estonian HEI with nearly 13,000 students, including 1,200 international students from 90 countries. The HEI consists of four faculties: Arts and Humanities, Medicine, Social Sciences, and Science and Technology. One of the aims of the dashboard was to communicate student-related information to academic stakeholders, such as program directors and career counsellors, to support the early detection of students at risk of dropping out from their studies. To design the dashboard, we followed a participatory approach, conducting a series of workshops to gather insight from the end-users. A participatory approach aims to directly and actively involve stakeholders in the design of a product (this refers to either a digital or a tangible artifact or even to a work process) to ensure that the final design takes the stakeholders’ requirements and perceptions into account and satisfies the stakeholders’ needs (Muller & Kuhn, 1993; Kensing & Blomberg, 1998). Here, the task was twofold: on the one hand, we wanted to gather requirements on work processes and strategies for supporting students at risk; on the other hand, we wanted to identify data-informed indicators that can support stakeholders in identifying students at risk. The rationale was that the stakeholders are responsible for identifying bottlenecks, for redesigning academic curricula, and for providing appropriate support to students. Therefore, they would be able to contribute to the strategic design of the IA infrastructure using their experience and expertise. We opened a call for the workshops, and 13 institutional stakeholders from five different specializations were selected to participate after volunteering. When choosing participants, we aimed at a representative population among bachelor’s and master’s curriculums and faculties (see Table 1) and stakeholders’ roles—in this case, program directors, academic specialists, and study counsellors.  Table 1.   Overview of the 13 Participants in the Study’s Five Focus Groups along with Information Regarding Their Faculty, Institutional Role (Academic Specialist, Program Director, or Career Counsellor), and Professional Experience (in Years) Participant   Faculty/department   Position   Years of experience in current position ST.C1   Science and Technology   Academic specialist   2 ST.C2   Science and Technology   Program director   2 AH1.C1   Arts and Humanities   Academic specialist   20 AH1.C2   Arts and Humanities   Program director   5 AH1.C3   Arts and Humanities   Program director   – CS.C1   Counselling Service   Career counsellor   1 CS.C2   Counselling Service   Career counsellor   2 CS.C3   Counselling Service   Academic specialist   10 AH2.C1   Arts and Humanities   Program director   20 AH2.C2   Arts and Humanities   Academic specialist   20 AH2.C3   Arts and Humanities   Academic specialist   20 M.C1   Medicine   Program director   3 M.C2   Medicine   Academic specialist   3 Academic specialists are institutional employees (either faculty or administrative staff) who are involved in institutional-level decision-making activities. Program directors are faculty members responsible for making decisions to support curriculum improvements and management. Career counsellors are qualified HEI employees who help students make decisions related to their work and education, plan and develop their career, and develop their job-searching skills. In addition, we selected participants to represent all faculties, participants who were fluent in English, and participants who stated that they had some basic knowledge about LA and dashboards. During the workshops, we used a semi-structured interview protocol (see Appendix B, Table 6) to guide the activity and to gain insight and information from the participants. The focus groups were conducted by an experienced researcher in human-computer interaction who has conducted several workshops and focus groups using qualitative research methods. We conducted five focus groups for stakeholders from five specializations. By conducting the focus groups, we aimed to uncover participants’ perceptions regarding dropout factors for this specific socio-cultural context and participants’ needs for supporting students at risk. The main aim of choosing focus groups instead of individual interviews was to enable peer discussions on institutional strategies that could potentially reveal different perspectives and priorities among stakeholders. The rationale was that these discussions would allow stakeholders to reflect on existing practices, to pinpoint challenges and limitations that could halt the adoption of IA solutions, and to envision benefits of this adoption, while, at the same time, the participants would have the opportunity to be exposed to different perspectives and to elaborate using arguments from their own experience. At the end  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   184

Page 8:
Finally, we performed LCA on the codes extracted from the content analysis. Our goal was to identify the underlying rela- tionships and structures among the different factors associated with student dropout as evidenced by participants’ perspectives (McCutcheon, 2002). Even though thematic analysis is a widely used technique in qualitative data, we selected content analysis due to its ability to quantify data by showing word patterns used in this context. LCA is a modelling method for identifying latent groups or subgroups based on pattern similarity in multivariate categorical data (Masyn, 2013). LCA is considered a “person-centric” model that focuses on respondent patterns, as opposed to “variable-centric” methods such as factor analysis, which are appropriate for continuous variables (McCutcheon, 2002). Here, we used LCA to identify latent relationships among dropout factors based on the similarity of participants’ responses in focus groups. We selected LCA due to its applicability to categorical data, allowing us to identify sets of underlying subgroups of individual factors based on the intersection of multiple observed behaviours (Lanza & Rhoades, 2013), that is, the participants’ responses. Further, LCA results are not sample dependent and can be replicated in other examples. At the same time, LCA is a widely used method of identifying reasons for high school dropout (Boyce & Bowers, 2016; Bowers & Sprott, 2012) and in the education context in general (Graves & Bowers, 2018).  4. Results  This section presents the results from the descriptive analysis, the qualitative analysis of the focus groups, the LCA analysis, and class interpretations based on participants’ perceptions.  4.1 Descriptive Statistics on Content Analysis  Table 2 presents the codes’ reference frequency, that is, how often participants mentioned the different codes during the focus groups. The results reflect the institutional stakeholders’ perception of student dropout and the diverse role of the HEI in dropout. Most participants agreed that there is a high number of dropouts in every specialization. However, participants were divided as to whether dropout is a significant problem for the HEI (12) or whether dropout is not necessarily a problem of the HEI (12). In the Faculty of Science and Technology (ST), five out of 12 statements declared dropout as a problem of the HEI and seven as not a problem of the HEI. ST has more students than the other faculties. Therefore, one could argue that the impact of dropout is not as significant or “visible” for this faculty as for other faculties of smaller size, and that this is reflected in the stakeholders’ perception. Eight participants from Arts and Humanities (AH1.C1, AH1.C2, AH1.C3), Counselling Services (CS.C1, CS.C2, CS.C3), and Medicine (M.C1 and M.C2) provided the greatest number of dropout reasons (Counselling Services = 11, Medicine = 10). Those participants belonged to faculties that had already designed, developed, and established strategies, policies, and individual practices to overcome dropout. AH1.C1, AH1.C2, and SC.CA3 (Arts and Humanities) and M.C1 and M.C2 (Medicine) stated that their specializations are less popular among students. Participants from Medicine noted that lack of popularity contributes significantly to early dropout. Therefore, they were concerned about establishing strategies to address student dropout. AH2 pointed out the usefulness of student data when designing such strategies. Our results suggest that the participants who have identified dropout patterns for their curricula have also reflected on strategies and policies to address dropout.  Table 2.   Frequencies of Responses Generated from Content Analysis for the Four Main Themes of This Study and per Focus Group  Main themes   ST   AH1   SC   AH2   M Dropout is a major concern   12   5   3   3   1 Yes but not the HEI’s problem   7   2   2   1   0 Yes, dropout is the HEI’s problem   5   3   1   2   1 Most important dropout patterns   8   7   11   5   10 Early dropouts   1   2   2   3   6 Middle dropouts   2   4   5   1   2 Final-year dropouts   5   1   4   1   2 Strategies, policies, or individual practices to address dropout 3   10   14   1   6 Established   2   10   6   1   4 Suggested   1   0   8   0   2 Students’ data can be used to ad- dress dropout 2   2   1   5   3  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   186

Page 18:
our results with documented dropouts and validation and triangulation of findings using other appropriate methods, such as epistemic network analysis (Z ̈ org ̋ o et al., 2021), will strengthen the outcomes of this work.  Acknowledgements  The authors would like to thank all of the participating institutional stakeholders for their support in this study.  Declaration of Conflicting Interest  The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.  Funding  This work was funded by the Estonian Research Council (PSG 286) and the European Union in the context of CEITER (grant agreement no. 669074).  References  Aguiar, E., Chawla, N. V., Brockman, J., Ambrose, G. A., & Goodrich, V. (2014). Engagement vs performance: Using electronic portfolios to predict first semester engineering student retention.   Proceedings of the Fourth International Conference on Learning Analytics and Knowledge   (LAK 2014), 24–28 March 2014, Indianapolis, IN, USA (pp. 103–112). ACM. https://doi.org/10.1145/2567574.2567583 Aguilar, S., Lonn, S., & Teasley, S. D. (2014). Perceptions and use of an early warning system during a higher education transition program.   Proceedings of the Fourth International Conference on Learning Analytics and Knowledge   (LAK 2014), 24–28 March 2014, Indianapolis, IN, USA, 113–117. https://doi.org/10.1145/2567574.2567625 Akaike, H. (1974). A new look at the statistical model identification.   IEEE Transactions on Automatic Control ,   19 (6), 716–723. https://doi.org/10.1109/TAC.1974.1100705 Ak c  ̧   apınar, G., Altun, A., & A s  ̧   kar, P. (2019). Using learning analytics to develop early-warning system for at-risk students.  International Journal of Educational Technology in Higher Education ,   16 (1), 129–147. https://doi.org/10.1186/s41239- 019-0172-z Alvarez, C. P., Martinez-Maldonado, R., & Buckingham Shum, S. (2020). LA-DECK: A card-based learning analytics co-design tool.   Proceedings of the 10th International Conference on Learning Analytics and Knowledge   (LAK 2020), 23–27 March 2020, Frankfurt, Germany (pp. 63–72). ACM. https://doi.org/10.1145/3375462.3375476 Ameri, S., Fard, M. J., Chinnam, R. B., & Reddy, C. K. (2016). Survival analysis based framework for early prediction of student dropouts.   Proceedings of the 25th ACM International Conference on Information and Knowledge Management   (CIKM 2016), 24–28 October 2016, Indianapolis, IN, USA (pp. 903–912). ACM. https://doi.org/10.1145/2983323.2983351 Arnold, K. E., & Pistilli, M. D. (2012). Course Signals at Purdue: Using learning analytics to increase student success.  Proceedings of the Second International Conference on Learning Analytics and Knowledge   (LAK 2012), 29 April–2 May 2012, Vancouver, BC, Canada (pp. 267–270). ACM. https://doi.org/10.1145/2330601.2330666 Barber, R., & Sharkey, M. (2012). Course correction: Using analytics to predict course success.   Proceedings of the Second International Conference on Learning Analytics and Knowledge   (LAK 2012), 29 April–2 May 2012, Vancouver, BC, Canada (pp. 259–262). ACM. https://doi.org/10.1145/2330601.2330664 Barefoot, B. O. (2004). Higher education’s revolving door: Confronting the problem of student drop out in US colleges and universities.   Open Learning: The Journal of Open, Distance and e-Learning ,   19 (1), 9–18. https://doi.org/10.1080/ 0268051042000177818 Bean, J. P. (1980). Dropouts and turnover: The synthesis and test of a causal model of student attrition.   Research in Higher Education ,   12 (2), 155–187. https://doi.org/10.1007/BF00976194 Beer, C., & Lawson, C. (2017). The problem of student attrition in higher education: An alternative perspective.   Journal of Further and Higher Education ,   41 (6), Article 4, 773–784. https://doi.org/10.1080/0309877X.2016.1177171 Behr, A., Giese, M., & Theune, K. (2020). Dropping out from higher education in Germany: An empirical evaluation of determinants for bachelor students.   Open Education Studies ,   2 (1), 126–148. https://doi.org/10.1515/edu-2020-0104 bin Mat, U., Buniyamin, N., Arsad, P. M., & Kassim, R. (2013). An overview of using academic analytics to predict and improve students’ achievement: A proposed proactive intelligent intervention.   Proceedings of the Fifth International Conference on Engineering Education   (ICEED 2013), 4–5 December 2013, Kuala Lumpur, Malaysia (pp. 126–130). IEEE. https://doi.org/10.1109/ICEED.2013.6908316  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   196

Page 19:
Bowers, A. J., & Sprott, R. (2012). Why 10th graders fail to finish high school: A dropout typology latent class analysis.   Journal of Education for Students Placed at Risk (JESPAR) ,   17 (3), 129–148. https://doi.org/10.1080/10824669.2012.692071 Boyce, J., & Bowers, A. J. (2016). Principal turnover: Are there different types of principals who move from or leave their schools? A latent class analysis of the 2007–2008 Schools and Staffing Survey and the 2008–2009 Principal Follow-Up Survey.   Leadership and Policy in Schools ,   15 (3), 237–272. https://doi.org/10.1080/15700763.2015.1047033 Brennan, J., Niccolo, D., & S    ́ en    ́ e, T. (2013).   Things we know and don’t know about the wider benefits of higher education: A review of the recent literature , Department for Business, Innovation and Skills, London School of Economics and Political Science. https://www.lse.ac.uk/business/consulting/reports/things-we-know-and-dont-know-about-the- wider-benefits-of-higher-education-a-review-of-the-recent-literature Cabrera, A. F., Nora, A., & Castaneda, M. B. (1992). The role of finances in the persistence process: A structural model.  Research in Higher Education ,   33 (5), 571–593. https://doi.org/10.1007/BF00973759 Carter, A. S., Hundhausen, C. D., & Adesope, O. (2015). The normalized programming state model: Predicting student performance in computing courses based on programming behavior.   Proceedings of the 11th Annual International Conference on International Computing Education Research   (ICER 2015), 9–13 July 2015, Omaha, NB, USA (pp. 141–150). ACM. https://doi.org/10.1145/2787622.2787710 Case, J. M., & Light, G. (2011). Emerging research methodologies in engineering education research.   Journal of Engineering Education ,   100 (1), 186–210. https://doi.org/10.1002/j.2168-9830.2011.tb00008.x Caulfield, M. (2013). What the Course Signals “kerfuffle” is about, and what it means to you.   EDUCAUSE.edu . https : //er.educause.edu/blogs/2013/11/what-the-course-signals-kerfuffle-is-about-and-what-it-means-to-you Chen, R. (2012). Institutional characteristics and college student dropout risks: A multilevel event history analysis.   Research in Higher Education ,   53 (5), 487–505. https://doi.org/10.1007/s11162-011-9241-4 Chou, C. - Y., Tseng, S. - F., Chih, W.   - C., Chen, Z.   - H., Chao, P. -   Y., Lai, K. R., Chan, C. - L., Yu, L. - C., & Lin, Y. - L. (2015). Open student models of core competencies at the curriculum level: Using learning analytics for student reflection.   IEEE Transactions on Emerging Topics in Computing ,   5 (1), 32–44. https://doi.org/10.1109/TETC.2015.2501805 Chounta, I., Pedaste, M., & Saks, K. (2019). Behind the scenes: Designing a learning analytics platform for higher education.  Companion Proceedings of the Ninth International Conference on Learning Analytics and Knowledge   (LAK 2019), 4–8 March 2019, Tempe, AZ, USA. https : / / www. solaresearch . org / core / companion - proceedings - of - the - 9th - international-learning-analytics-and-knowledge-conference-lak19/ Chounta, I. -   A., Uiboleht, K., Roosim    ̈ ae, K., Pedaste, M., & Valk, A. (2020). From data to intervention: Predicting students at-risk in a higher education institution.   Companion Proceedings of the 10th International Conference on Learning Analytics and Knowledge   (LAK 2020), 23–27 March 2020, Frankfurt, Germany. https://www.solaresearch.org/core/lak20- companion-proceedings/ Clow, D. (2012). The learning analytics cycle: Closing the loop effectively.   Proceedings of the Second International Conference on Learning Analytics and Knowledge   (LAK 2012), 29 April–2 May 2012, Vancouver, BC, Canada (pp. 134–138). ACM. https://doi.org/10.1145/2330601.2330636 Colvin, C., Rogers, T., Wade, A., Dawson, S., Gasevic, D., Buckingham Shum, S., Nelson, K., Alexander, S., Lockyer, L., Kennedy, G., Corrin, L., & Fisher, J. (2015).   Student retention and learning analytics: A snapshot of Australian practice and a framework for advancement . Australian Government Office for Learning and Teaching. https://research. usc.edu.au/esploro/outputs/report/Student-retention-and-learning-analytics-A/99449564202621 Conijn, R., Snijders, C., Kleingeld, A., & Matzat, U. (2017). Predicting student performance from LMS data: A comparison of 17 blended courses using Moodle LMS.   IEEE Transactions on Learning Technologies ,   10 (1), 17–29. https : //doi.org/10.1109/TLT.2016.2616312 Crosling, G., Heagney, M., & Thomas, L. (2009). Improving student retention in higher education: Improving teaching and learning.   Australian Universities’ Review ,   51 (2), 9–18. https://aur.nteu.org.au/wp-content/uploads/2021/08/aur 51- 02.pdf Daud, A., Aljohani, N. R., Abbasi, R. A., Lytras, M. D., Abbas, F., & Alowibdi, J. S. (2017). Predicting student performance using advanced learning analytics.   Proceedings of the 26th International Conference on World Wide Web Companion  (WWW 2017 Companion), 3–7 April 2017, Perth, Australia (pp. 415–421). International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/3041021.3054164 Dawson, S., Poquet, O., Colvin, C., Rogers, T., Pardo, A., & Gasevic, D. (2018). Rethinking learning analytics adoption through complexity leadership theory.   Proceedings of the Eighth International Conference on Learning Analytics and Knowledge   (LAK 2018), 5–9 March 2018, Sydney, Australia (pp. 236–244). ACM. https://doi.org/10.1145/3170358. 3170375 De Silva, L. M. H., Rodr  ́ ıguez-Triana, M. J., Chounta, I. - A., Tammets, K., & Shankar, S. K. (2020). Curriculum analytics as a communication mediator among stakeholders to enable the discussion and inform decision-making.   Companion  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   197

Page 20:
Proceedings of the 10th International Conference on Learning Analytics and Knowledge   (LAK 2020), 23–27 March 2020, Frankfurt, Germany (pp. 181–186). https://www.solaresearch.org/core/lak20-companion-proceedings/ Dollinger, M., Liu, D., Arthars, N., & Lodge, J. (2019). Working together in learning analytics towards the co-creation of value.  Journal of Learning Analytics ,   6 (2), 10–26. https://doi.org/10.18608/jla.2019.62.2 Falc    ̃ ao, T. P., Mello, R. F., Rodrigues, R. L., Diniz, J. R. B., Tsai, Y. - S., & Ga   ˇ sevi    ́ c, D. (2020). Perceptions and expectations about learning analytics from a Brazilian higher education institution.   Proceedings of the 10th International Conference on Learning Analytics and Knowledge   (LAK 2020), 23–27 March 2020, Frankfurt, Germany (pp. 240–249). ACM. https://doi.org/10.1145/3375462.3375478 Fei, M., & Yeung, D. -   Y. (2015). Temporal models for predicting student dropout in massive open online courses.   2015 IEEE International Conference on Data Mining Workshop   (ICDMW 2015), 14–17 November 2015, Atlantic City, NJ, USA (pp. 256–263). IEEE. https://doi.org/10.1109/ICDMW.2015.174 Finn, A. S., Kraft, M. A., West, M. R., Leonard, J. A., Bish, C. E., Martin, R. E., Sheridan, M. A., Gabrieli, C. F. O., & Gabrieli, J. D. E. (2014). Cognitive skills, student achievement tests, and schools.   Psychological Science ,   3 , Article 25, 736–744. https://doi.org/10.1177/0956797613516008 Foti, R. J., Bray, B. C., Thompson, N. J., & Allgood, S. F. (2012). Know thy self, know thy leader: Contributions of a pattern-oriented approach to examining leader perceptions [Leadership and Individual Differences].   The Leadership Quarterly ,   23 (4), 702–717. https://doi.org/10.1016/j.leaqua.2012.03.007 Gansemer-Topf, A. M., & Schuh, J. H. (2006). Institutional selectivity and institutional expenditures: Examining organizational factors that contribute to retention and graduation.   Research in Higher Education ,   47 (6), 613–642. https://doi.org/10. 1007/s11162-006-9009-4 Georg, W. (2009). Individual and institutional factors in the tendency to drop out of higher education: A multilevel analysis using data from the Konstanz Student Survey.   Studies in Higher Education ,   34 (6), 647–661. https://doi.org/10.1080/ 03075070802592730 Giannakos, M. N., Pappas, I. O., Jaccheri, L., & Sampson, D. G. (2017). Understanding student retention in computer science education: The role of environment, gains, barriers and usefulness.   Education and Information Technologies ,   22 (5), 2365–2382. https://doi.org/10.1007/s10639-016-9538-1 Gilliot, J.   -   M., Iksal, S., Medou, D. M., & Dabbebi, I. (2018). Participatory design of learning analytics dashboards.   Proceedings of the 30th Conference on l’Interaction Homme-Machine   (IHM 2018), 23–26 October 2018, Brest, France (pp. 119– 127). ACM. https://doi.org/10.1145/3286689.3286693 Gkontzis, A. F., Kotsiantis, S., Tsoni, R., & Verykios, V. S. (2018). An effective LA approach to predict student achievement.  Proceedings of the 22nd Pan-Hellenic Conference on Informatics   (PCI 2018), 29 November–1 December 2018, Athens, Greece (pp. 76–81). ACM. https://doi.org/10.1145/3291533.3291551 Graves, K., & Bowers, A. (2018). Toward a typology of technology-using teachers in the “new digital divide”: A latent class analysis (LCA) of the NCES fast response survey system teachers’ use of educational technology in US public schools, 2009 (FRSS 95).   Teachers College Record ,   120 (8), 1–42. https://www.tcrecord.org/Content.asp?ContentId=22277 Herbaut, E. (2021). Overcoming failure in higher education: Social inequalities and compensatory advantage in dropout patterns.  Acta Sociologica ,   64 (4), 383–402. https://doi.org/10.1177/0001699320920916 Herodotou, C., Naydenova, G., Boroowa, A., Gilmour, A., & Rienties, B. (2020). How can predictive learning analytics and motivational interventions increase student retention and enhance administrative support in distance education?  Journal of Learning Analytics ,   7 (2), 72–83. https://doi.org/10.18608/jla.2020.72.4 Hilliger, I., Aguirre, C., Miranda, C., Celis, S., & P    ́ erez-Sanagustın, M. (2020). Design of a curriculum analytics tool to support continuous improvement processes in higher education.   Proceedings of the 10th International Conference on Learning Analytics and Knowledge   (LAK 2020), 23–27 March 2020, Frankfurt, Germany (pp. 181–186). ACM. https://doi.org/10.1145/3375462.3375489 Hilliger, I., Ortiz-Rojas, M., Pes    ́ antez-Cabrera, P., Scheihing, E., Tsai, Y. - S., Mu    ̃ noz-Merino, P. J., Broos, T., Whitelock- Wainwright, A., Ga   ˇ sevi    ́ c, D., & P    ́ erez-Sanagust  ́ ın, M. (2020). Towards learning analytics adoption: A mixed methods study of data-related practices and policies in Latin American universities.   British Journal of Educational Technology ,  51 (4), 915–937. https://doi.org/10.1111/bjet.12933 Hilliger, I., Ortiz-Rojas, M., Pes    ́ antez-Cabrera, P., Scheihing, E., Tsai, Y. - S., Mu    ̃ noz-Merino, P. J., Broos, T., Whitelock- Wainwright, A., & P    ́ erez-Sanagust  ́ ın, M. (2020). Identifying needs for learning analytics adoption in Latin American universities: A mixed-methods approach.   The Internet and Higher Education ,   45 , 100726. https://doi.org/10.1016/j. iheduc.2020.100726 Hinton, L. (2007). Causes of attrition in first year students in science foundation courses and recommendations for intervention.  Studies in Learning, Evaluation, Innovation and Development ,   4 (2), 13–26.  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   198

Page 21:
Hoffman, M., Richmond, J., Morrow, J., & Salomone, K. (2002). Investigating “sense of belonging” in first-year college students.   Journal of College Student Retention ,   4 , 227–256. https://doi.org/10.2190/DRYC-CXQ9-JQ8V-HT4V Hovdhaugen, E. (2009). Transfer and dropout: Different forms of student departure in Norway.   Studies in Higher Education ,  34 (1), 1–17. https://doi.org/10.1080/03075070802457009 Howell, J. A., Roberts, L. D., Seaman, K., & Gibson, D. C. (2018). Are we on our way to becoming a “helicopter university”? Academics’ views on learning analytics.   Technology, Knowledge and Learning ,   23 (1), 1–20. https://doi.org/10.1007/ s10758-017-9329-9 Ifenthaler, D. (2015). Learning analytics. In J. M. Spector (Ed.),   The SAGE Encyclopedia of Educational Technology . SAGE Knowledge. https://doi.org/10.4135/9781483346397 Ifenthaler, D., & Yau, J. Y. - K. (2020). Utilising learning analytics to support study success in higher education: A systematic review.   Educational Technology Research and Development ,   68 (1), 1–30. https://doi.org/10.1007/s11423-020-09788-z Jaggo, I. (2020).   Katkestamine ja v    ̈ aljalangemine k    ̃ orghariduses 2011/12–2018/19 . Haridus-ja Teadusministeerium. https: //www.hm.ee/sites/default/files/katkestamise analuus ingrid jaggo.pdf Johnson, I. (2008). Enrollment, persistence and graduation of in-site students at a public research university: Does high school matter?   Research in Higher Education ,   49 , 776–793. https://doi.org/10.1007/s11162-008-9105-8 Jung, J., & Kim, Y. (2018). Exploring regional and institutional factors of international students’ dropout: The South Korea case.   Higher Education Quarterly ,   72 (2), 141–159. https://doi.org/10.1111/hequ.12148 Kaplan, R., James, J., Figueroa, D. T., Rawkins, C., & Dumont, C. (2020).   Education policy outlook Estonia . OECD. https: //www.oecd.org/education/policy-outlook/country-profile-Estonia-2020.pdf Kensing, F., & Blomberg, J. (1998). Participatory design: Issues and concerns.   Computer Supported Cooperative Work (CSCW) ,  7 (3), 167–185. https://doi.org/10.1023/A:1008689307411 Kollom, K., Tammets, K., Scheffel, M., Tsai, Y. -   S., Jivet, I., Mu    ̃ noz-Merino, P. J., Moreno-Marcos, P. M., Whitelock-Wainwright, A., Calleja, A. R., Gasevic, D., Kloos, C. D., Drachsler, H., & Ley, T. (2021). A four-country cross-case analysis of academic staff expectations about learning analytics in higher education.   The Internet and Higher Education ,   49 , 100788. https://doi.org/10.1016/j.iheduc.2020.100788 Kori, K., Pedaste, M., Altin, H., T    ̃ onisson, E., & Palts, T. (2016). Factors that influence students’ motivation to start and to continue studying information technology in Estonia.   IEEE Transactions on Education ,   59 (4), 255–262. https: //doi.org/10.1109/TE.2016.2528889 Lanza, S. T., & Rhoades, B. L. (2013). Latent class analysis: An alternative perspective on subgroup analysis in prevention and treatment.   Prevention Science ,   14 (2), 157–168. https://doi.org/10.1007/s11121-011-0201-1 Li, I., & Dockery, A. M. (2015). Does schools’ socio-economic status influence university outcomes?   The Australian Journal of Labour Economics ,   18 (1), 75–94. http://hdl.handle.net/20.500.11937/19422 Li, I. W., & Carroll, D. R. (2020). Factors influencing dropout and academic performance: An Australian higher education equity perspective.   Higher Education Policy and Management ,   42 (1), 14–30. https://doi.org/10.1080/1360080X.2019. 1649993 Liaqat, A., Axtell, B., Munteanu, C., & Epp, C. D. (2018). Contextual inquiry, participatory design, and learning analytics.  Companion Proceedings of the Eighth International Conference on Learning Analytics and Knowledge   (LAK 2018), 5–9 March 2018, Sydney, Australia. http://bit.ly/lak18-companion-proceedings/ Luzeckyj, A., West, D. S., Searle, B. K., Toohey, D. P., Vanderlelie, J. J., & Bell, K. R. (2020). Stakeholder perspectives (staff and students) on institution-wide use of learning analytics to improve learning and teaching outcomes. In D. Ifenthaler & D. Gibson (Eds.),   Adoption of data analytics in higher education learning and teaching   (pp. 177–200). Springer International. https://doi.org/10.1007/978-3-030-47392-1 10 Masyn, K. E. (2013). Latent class analysis and finite mixture modeling. In T. D. Little (Ed.),   The oxford handbook of quantitative methods in psychology: Vol. 2: Statistical analysis . Oxford University Press. https://doi.org/10.1093/oxfordhb/ 9780199934898.013.0025 Maxwell, J. A. (2012).   Qualitative research design: An interactive approach   (3rd edition). SAGE Publishing. https://us.sagepub. com/en-us/nam/qualitative-research-design/book234502 McCutcheon, A. L. (2002). Basic concepts and procedures in single- and multiple-group latent class analysis. In J. A. Hagenaars & A. L. McCutcheon (Eds.),   Applied latent class analysis   (pp. 56–86). Cambridge University Press. https://doi.org/10.1017/CBO9780511499531.003 McHugh, M. L. (2012). Interrater reliability: The kappa statistic.   Biochemia Medica ,   22 (3), 276–282. https://doi.org/10.11613/ BM.2012.031 Mitra, S., & Goldstein, Z. (2015). Designing early detection and intervention techniques via predictive statistical models—A case study on improving student performance in a business statistics course.   Communications in Statistics: Case Studies, Data Analysis and Applications ,   1 (1), 9–21. https://doi.org/10.1080/23737484.2015.1063409  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   199

Page 22:
Muller, M. J., & Kuhn, S. (1993). Participatory design.   Communications of the ACM ,   36 (6), 24–29. https://doi.org/10.1145/ 153571.255960 Nespereira, C. G., Vilas, A. F., & Redondo, R. P. D. (2015). Am I failing this course? Risk prediction using e-learning data.  Proceedings of the Third International Conference on Technological Ecosystems for Enhancing Multiculturality  (TEEM 2015), 7–9 October 2015, Porto, Portugal (pp. 271–276). ACM. https://doi.org/10.1145/2808580.2808621 Nguyen, A., Gardner, L., & Sheridan, D. (2020). A design methodology for learning analytics information systems: Informing learning analytics development with learning design.   Proceedings of the 53rd Hawaii International Conference on System Sciences   (HICSS 2020), 7–10 January 2020, Grand Wailea, Maui, HI, USA (pp. 108–117). ScholarSpace. https://doi.org/10.24251/HICSS.2020.014 Niitsoo, M., Paales, M., Pedaste, M., Siiman, L., & T    ̃ onisson, E. (2014). Predictors of informatics students’ progress and graduation in university studies. In L. G. Chova, A. L. Mart  ́ ınez, & I. C. Torres (Eds.),   Proceedings of the Eighth International Conference on Technology, Education and Development   (INTED 2014), 10–12 March 2014, Valencia, Spain (pp. 2521–2529). IATED Academy. https://library.iated.org/view/NIITSOO2014PRE OECD. (2019).   Education at a glance 2019: OECD indicators . OECD Publishing. https://doi.org/10.1787/f8d7880d-en Okubo, F., Yamashita, T., Shimada, A., & Ogata, H. (2017). A neural network approach for students’ performance prediction.  Proceedings of the Seventh International Conference on Learning Analytics and Knowledge   (LAK 2017), 13–17 March 2017, Vancouver, BC, Canada (pp. 598–599). ACM. https://doi.org/10.1145/3027385.3029479 Papamitsiou, Z., & Economides, A. A. (2015). Temporal learning analytics visualizations for increasing awareness during assessment.   International Journal of Educational Technology in Higher Education ,   12 (3), 129–147. https://doi.org/10. 7238/rusc.v12i3.2519 Pascarella, E. T., & Terenzini, P. T. (2005).   How college affects students: A third decade of research   (Vol. 2). Wiley. Patti, M. V., Tarpley, R. S., Goree, C. T., & Tice, G. E. (1993). The relationship of college facilities and services to student retention. Paper presented at the 1993 Annual Meeting of the Mid-South Educational Research Association, 10–12 November 1993, New Orleans, LA, USA. https://files.eric.ed.gov/fulltext/ED368312.pdf Rogers, T., Colvin, C., & Chiera, B. (2014). Modest analytics: Using the index method to identify students at risk of failure.  Proceedings of the Fourth International Conference on Learning Analytics and Knowledge   (LAK 2014), 24–28 March 2014, Indianapolis, IN, USA, 118–122. https://doi.org/10.1145/2567574.2567629 Romero, C., & Ventura, S. (2020). Educational data mining and learning analytics: An updated survey.   Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery ,   10 (3), 89–96. https://doi.org/10.1002/widm.1355 Ryan, J. F. (2004). The relationship between institutional expenditures and degree attainment at baccalaureate colleges.   Research in Higher Education ,   45 (2), 97–114. https://doi.org/10.1023/B:RIHE.0000015691.02545.61 Schwarz, G. (1978). Estimating the dimension of a model.   The Annals of Statistics ,   6 (2), 461–464. https://doi.org/10.1214/aos/ 1176344136 Seymour, E., & Hewitt, N. M. (1997).   Talking about leaving: Why undergraduates leave the sciences   (1st edition). Westview Press. Shehata, S., & Arnold, K. E. (2015). Measuring student success using predictive engine.   Proceedings of the Fifth International Conference on Learning Analytics and Knowledge   (LAK 2015), 16–20 March 2015, Poughkeepsie, NY, USA (pp. 416– 417). ACM. https://doi.org/10.1145/2723576.2723661 Siemens, G., & Long, P. (2011). Penetrating the fog: Analytics in learning and education.   EDUCAUSE Review ,   46 (5), 30–32. https://er.educause.edu/articles/2011/9/penetrating-the-fog-analytics-in-learning-and-education Sønderlund, A. L., Hughes, E., & Smith, J. (2019). The efficacy of learning analytics interventions in higher education: A systematic review.   British Journal of Educational Technology ,   50 (5), 2594–2618. https://doi.org/10.1111/bjet.12720 Spady, W. G. (1970). Dropouts from higher education: Toward an empirical model.   Interchange ,   2 , 38–62. https://doi.org/10. 1007/BF02282469 Star, M., & Collette, L. (2010). GPS: Shaping student success one conversation at a time.   Educause Quarterly ,   33 (4). https://www.learntechlib.org/p/109170 Sun, K., Mhaidli, A. H., Watel, S., Brooks, C. A., & Schaub, F. (2019). It’s my data! tensions among stakeholders of a learning analytics dashboard.   Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems   (CHI 2019), 4–9 May 2019, Glasgow, UK (pp. 1–14). ACM. https://doi.org/10.1145/3290605.3300824 Tarmazdi, H., Vivian, R., Szabo, C., Falkner, K., & Falkner, N. (2015). Using learning analytics to visualise computer science teamwork.   Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education  (ITICSE 2015), 4–8 July 2015, Vilnius, Lithuania, 165–170. https://doi.org/10.1145/2729094.2742613 Tinto, V. (1975). Dropout from higher education: A theoretical synthesis of recent research.   Review of Educational Research ,  45 (1), 89–125. https://doi.org/10.3102/00346543045001089  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   200

Page 23:
Titus, M. A. (2006). Understanding college degree completion of students with low socioeconomic status: The influence of the institutional financial context.   Research in Higher Education ,   47 (4), 371–398. https://doi.org/10.1007/s11162-005- 9000-5 Tsai, Y. -   S., & Gasevic, D. (2017). Learning analytics in higher education—Challenges and policies: A review of eight learning analytics policies.   Proceedings of the Seventh International Conference on Learning Analytics and Knowledge   (LAK 2017), 13–17 March 2017, Vancouver, BC, Canada (pp. 233–242). ACM. https://doi.org/10.1145/3027385.3027400 Tsai, Y. -   S., Scheffel, M., & Gasevic, D. (2017). SHEILA policy framework—Informing institutional strategies and policy processes of learning analytics.   Proceedings of the Eighth International Conference on Learning Analytics and Knowledge   (LAK 2018), 5–9 March 2018. ACM. https://doi.org/10.1145/3170358.3170367 Ullah, M. A., Alam, M. M., Mahiuddin, M., & Rahman, M. M. (2019). Predicting factors of students dissatisfaction for retention. In A. Abraham, P. Dutta, J. K. Mandal, A. Bhattacharya, & S. Dutta (Eds.),   Emerging technologies in data mining and information security   (pp. 501–510). Springer. https://doi.org/10.1007/978-981-13-1951-8 45 Vezzoli, Y., Mavrikis, M., & Vasalou, A. (2020). Inspiration cards workshops with primary teachers in the early co-design stages of learning analytics.   Proceedings of the 10th International Conference on Learning Analytics and Knowledge  (LAK 2020), 23–27 March 2020, Frankfurt, Germany (pp. 73–82). ACM. https://doi.org/10.1145/3375462.3375537 Vogel, C., Hochberg, J., Hackstein, S., Bockshecker, A., Bastiaens, T. J., & Baum    ̈ ol, U. (2018). Dropout in distance education and how to prevent it. In T. Bastiaens (Ed.),   Proceedings of EdMedia + Innovate Learning 2018,   25–29 June 2018, Amsterdam, Netherlands (pp. 1788–1799). Association for the Advancement of Computing in Education (AACE). https://www.learntechlib.org/p/184409/ Vossensteyn, J. J., Kottmann, A., Jongbloed, B. W., Kaiser, F., Cremonini, L., Stensaker, B., Hovdhaugen, E., & Wollscheid, S. (2015).   Dropout and completion in higher education in Europe: Main report . European Union. https://doi.org/10. 2766/826962 West, D., Heath, D., Huijser, H., Lizzio, A., Toohey, D., Miles, C., Searle, B., & Bronnimann, J. (2015).   Learning ana- lytics: Assisting universities with student retention . Australian Government Office for Learning; Teaching. http : //researchrepository.murdoch.edu.au/id/eprint/35134 West, D., Luzeckyj, A., Toohey, D., Vanderlelie, J., & Searle, B. (2020). Do academics and university administrators really know better? The ethics of positioning student perspectives in learning analytics.   Australasian Journal of Educational Technology ,   36 (2), 60–70. https://doi.org/10.14742/ajet.4653 Wild, S., & Heuling, L. S. (2020). Student dropout and retention: An event history analysis among students in cooperative higher education.   International Journal of Educational Research ,   104 . https://doi.org/10.1016/j.ijer.2020.101687 Willcoxson, L., Cotter, J., & Joy, S. (2011). Beyond the first-year experience: The impact on attrition of student experiences throughout undergraduate degree studies in six diverse institutions.   Studies in Higher Education ,   36 (3), 331–352. https://doi.org/10.1080/03075070903581533 Wolff, A., Zdrahal, Z., Herrmannova, D., Kuzilek, J., & Hlosta, M. (2014). Developing predictive models for early detection of at-risk students on distance learning modules.   Machine Learning and Learning Analytics Workshop at the Fourth International Conference on Learning Analytics and Knowledge   (LAK 2014), 24–28 March 2014, Indianapolis, IN, USA. http://lak14indy.wordpress.com/ Wong, W. Y., & Lavrencic, M. (2016). Using a risk management approach in analytics for curriculum and program quality improvement. In J. Greer, M. Molinaro, X. Ochoa, & T. McKay (Eds.),   Proceedings of the First Learning Analytics for Curriculum and Program Quality Improvement Workshop   (PCLA 2016), 25 April 2016, Edinburgh, Scotland (pp. 10–14). CEUR. http://ceur-ws.org/Vol-1590/paper-02.pdf Worsley, J., Pennington, A., & Corcoran, R. (2020).   What interventions improve college and university students’ mental health and wellbeing? A review of review-level evidence   (tech. rep.). University of Liverpool. https://livrepository.liverpool. ac.uk/3089948/ Xenosa, M., Pierrakeasa, C., & Pintelas, P. (2002). A survey on student dropout rates and dropout causes concerning the students in the Course of Informatics of the Hellenic Open University.   Computers & Education ,   39 (4), 361–377. https://doi.org/10.1016/S0360-1315(02)00072-6 Yeager, D. S., Hanselman, P., Walton, G. M., Murray, J. S., Crosnoe, R., Muller, C., Tipton, E., Schneider, B., Hulleman, C. S., Hinojosa, C. P., Paunesku, D., Romero, C., Flint, K., Roberts, A., Trott, J., Iachan, R., Buontempo, J., Yang, S. M., Carvalho, C. M., . . . Dweck, C. S. (2019). A national experiment reveals where a growth mindset improves achievement.   Nature ,   573 (7774), 364–369. https://doi.org/10.1038/s41586-019-1466-y Z    ̈ org    ̋ o, S., Swiecki, Z., & Ruis, A. R. (2021). Exploring the effects of segmentation on semi-structured interview data with epistemic network analysis. In A. R. Ruis & S. B. Lee (Eds.),   Advances in quantitative ethnography   (pp. 78–90). Springer International. https://doi.org/10.1007/978-3-030-67788-6 6  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   201

Page 10:
Table 3.   Overview of Classes Derived from the LCA  No.   Class name   Student aspects   University aspects   NC 1   Personal dropout rea- sons Students’ self-motivated intentions to dropout 1 2   Interest in the subjects   Wrong specialty   Lack of alignment between personal and curriculum goals 2 3   Curriculum alignment with satisfaction Talk to students, curriculum devel- opment 2 4   Institutional support of students goal commit- ment Identify   study   priorities/choices made in the admission, inform rele- vant people to take actions 2 5   Individual   academic struggles Individual learning difficulties, low grades, failed courses 2 6   Professional and finan- cial concerns Financial issues, employment   2 7   Academic integration   Opportunity to transfer, admission score, credits for next and previous semester, next semester payments Misused opportunities   5 8   Student well-being   Uncertainty   about   future   profes- sional opportunities, health issues, extracurricular over-involvement Adapt to/raise awareness of student background,   detect/monitor   less- engaged students, academic support programs 5 9   Faculty-student   rela- tionship Report   on   not-interesting   study paths Supporting students is an institu- tional   responsibility,   counselling, group discussions, waste of invest- ments due to dropouts 6 10   Social   support   and sense of stability Psychological issues, social issues, less support from university, study results Keep   track   of   graduates   and dropouts,   encourage   student- supervisor communication, level of the university 7 11   Curriculum-related difficulties Hard curriculum, struggling with thesis, negative student-supervisor relationship,   extreme   workload, course   registrations,   academic leaves Seminar and courses, study and self- management skills 8 12   Personal and contex- tual aspects Academic   adjustment   of international-national   students, family issues, political problems, qualification-oriented targets, per- fectionism perceived by national students Tax office data to collect personal information, qualitative perspectives of reasons for dropout, no negative consequences after dropout, benefits from degree completion 9  For each class, the factors are grouped according to whether they are student or university related, and the number of codes (NC) belonging to the class is provided.  Participants pointed to the importance of peer mentoring. According to participants’ experience, there are situations where students decide to go for a particular specialization, and after several weeks they drop out. When talking to the participants, students mentioned that “it is not easy to get understanding [in] this one big discipline.” •   Class 4—Institutional support of students’ goal commitment:   Students’ goals and commitment have a significant impact on dropout. Some students enter HEIs with clear goals, and the first choice of admission usually reflects this. Thus, admission choice can be a predictor of dropout in the early stage of studies. Taking into account students’ admission choice, academic stakeholders can initiate discussions of students’ choice of specialization. ([AH1.C1] “If somebody is not doing well, I communicate with the program manager or program director usually; if it’s a first-year student, I still contact the members of teaching staff and and ask whether the student has been in seminars and prepared for the seminars, and so on.”) •   Class 5—Individual academic struggles:   Learning is a factor that determines students’ persistence. Some students need more time than others to acclimatize to academic life. Students who begin studying without understanding or being aware of their abilities may get frightened by the demands of academic life. ([M.C1] “Since they’re not expected to do that much during the semester, and then at some point their workload may skyrocket.”) The student’s pass/fail status and course completion level are potential data indicators for predicting their academic struggles. •   Class 6—Professional and financial concerns:   Participants suggested that employment is one of the most prominent  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)   188

Page 24:
Appendix A: Codes  Table 5.   Codes Generated from Content Analysis  Code   R1   R2   Kappa  Student Dropouts are a major concern or not Students Self-Motivated Intentions to Dropout   6   6   1 Political Problems   1   1   1 No negative consequences after dropout   4   4   1 Benefits from Degree Completion   1   1   1 Qualification-Oriented Targets   1   1   1 Lack of alignment between personal and curriculum goals   5   5   1 Supporting Students is an Institutional Responsibility   2   2   1 Waste of Investments due to dropouts   3   3   1 Misused Opportunities   1   1   1 Level of the University   1   1   1 Reasons for dropouts Uncertainty About Future Professional Opportunities   2   2   1 Report on Not-Interesting Study Paths   4   4   1 Opportunity to Transfer   3   3   1 Wrong specialty   6   6   1 Extreme Workload   1   1   1 Family Issues   2   2   1 Financial Issues   2   2   1 Health Issues   1   1   1 Psychological Issues   1   1   1 Social Issues   4   4   1 Hard Curricular   6   6   1 Individual learning difficulties   3   3   1 Less Support form University   1   1   1 Employment   8   7   0.6 Struggling with Thesis   3   3   1 Negative Student-Supervisor Relationship   1   1   1 Perfectionism Perceived by National Students   1   1   1 Strategies established and suggested to address dropouts Detecting/Monitoring Less Engaged Students   1   1   1 Counselling   1   1   1 Academic Support Programs   3   3   1 Inform Relevant People to Take Actions   6   6   1 Keep a track of graduates and dropouts   1   1   1 Encourage Student-Supervisor Communication   1   1   1 Talk to students   12   11   0.84 Seminar and Courses   4   4   1 Study and Self Management Skills   2   2   1 Curriculum Development   6   6   1 Group Discussions   1   1   1 What data can use as an indicators Course Registrations   3   3   1 Study Results   2   2   1  ISSN 1929-7750 (online). The Journal of Learning Analytics works under a Creative Commons License, Attribution - NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0)

